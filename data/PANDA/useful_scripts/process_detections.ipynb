{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import json\n",
    "import numpy as np\n",
    "import os, cv2, glob, json, gc\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from moviepy.editor import VideoFileClip\n",
    "import skvideo.io\n",
    "from tqdm import tqdm\n",
    "import circstat as CS\n",
    "import scipy as sc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.float = np.float64\n",
    "np.int = np.int_\n",
    "\n",
    "# COCO limb sequence (0-based indexing)\n",
    "coco_limb_sequence = [\n",
    "    (0, 1),  # Nose to Left Eye\n",
    "    (0, 2),  # Nose to Right Eye\n",
    "    (1, 3),  # Left Eye to Left Ear\n",
    "    (2, 4),  # Right Eye to Right Ear\n",
    "    (5, 7),  # Left Shoulder to Left Elbow\n",
    "    (7, 9),  # Left Elbow to Left Wrist\n",
    "    (6, 8),  # Right Shoulder to Right Elbow\n",
    "    (8, 10), # Right Elbow to Right Wrist\n",
    "    (5, 6),  # Left Shoulder to Right Shoulder\n",
    "    (5, 11), # Left Shoulder to Left Hip\n",
    "    (6, 12), # Right Shoulder to Right Hip\n",
    "    (11, 12),# Left Hip to Right Hip\n",
    "    (11, 13),# Left Hip to Left Knee\n",
    "    (13, 15),# Left Knee to Left Ankle\n",
    "    (12, 14),# Right Hip to Right Knee\n",
    "    (14, 16) # Right Knee to Right Ankle\n",
    "]\n",
    "\n",
    "limb_sequence = [\n",
    "    (0,15),\n",
    "    (0,16),\n",
    "    (15,16),\n",
    "    (15,17),\n",
    "    (16,18),\n",
    "    (1,2),\n",
    "    (2,3),\n",
    "    (3,4),\n",
    "    (1,5),\n",
    "    (5,6),\n",
    "    (6,7),\n",
    "    (2,9),\n",
    "    (5,12),\n",
    "    (8,9),\n",
    "    (9,10),\n",
    "    (10,11),\n",
    "    (8,12),\n",
    "    (12,13),\n",
    "    (13,14)\n",
    "    ]\n",
    "\n",
    "mapping = {0:0,1:15,2:16,3:17,4:18,5:2,6:5,7:3,8:6,9:4,10:7,11:9,12:12,13:10,14:13,15:11,16:14}\n",
    "\n",
    "# COCO part list\n",
    "part_list = {\n",
    "    0: \"Nose\",\n",
    "    1: \"Left Eye\",\n",
    "    2: \"Right Eye\",\n",
    "    3: \"Left Ear\",\n",
    "    4: \"Right Ear\",\n",
    "    5: \"Left Shoulder\",\n",
    "    6: \"Right Shoulder\",\n",
    "    7: \"Left Elbow\",\n",
    "    8: \"Right Elbow\",\n",
    "    9: \"Left Wrist\",\n",
    "    10: \"Right Wrist\",\n",
    "    11: \"Left Hip\",\n",
    "    12: \"Right Hip\",\n",
    "    13: \"Left Knee\",\n",
    "    14: \"Right Knee\",\n",
    "    15: \"Left Ankle\",\n",
    "    16: \"Right Ankle\"\n",
    "}\n",
    "\n",
    "\n",
    "colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0],\n",
    "          [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255],\n",
    "          [85, 0, 255], \\\n",
    "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85],[255, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file(file_path, threshold=0.8):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    results = []\n",
    "    for frame in data:\n",
    "        frame_id = frame['frame_id']\n",
    "        if frame['instances']:  # Ensure there is at least one detection\n",
    "            first_instance = frame['instances'][0]\n",
    "            keypoint_scores = first_instance['keypoint_scores']\n",
    "            \n",
    "            if len(keypoint_scores) != 17:\n",
    "                continue\n",
    "            \n",
    "            all_above_thr = all(score > threshold for score in keypoint_scores)\n",
    "            results.append({\n",
    "                'frame_id': frame_id,\n",
    "                'first_detection_keypoints': first_instance['keypoints'],\n",
    "                'first_detection_confidence': keypoint_scores,\n",
    "                'all_keypoints_above_thr': all_above_thr\n",
    "            })\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def find_continuous_good_blocks(analysis_results):\n",
    "    good_blocks = []\n",
    "    current_block = []\n",
    "    \n",
    "    for result in analysis_results:\n",
    "        if result['all_keypoints_above_thr']:\n",
    "            current_block.append(result)\n",
    "        else:\n",
    "            if len(current_block) >= 30:\n",
    "                good_blocks.append(current_block)\n",
    "            current_block = []\n",
    "    \n",
    "    # Check if the last block in the sequence is a good block\n",
    "    if len(current_block) >= 30:\n",
    "        good_blocks.append(current_block)\n",
    "    \n",
    "    return good_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_keypoints(block, window_size=3):\n",
    "    \"\"\"\n",
    "    Apply rolling average smoothing to keypoints in a block.\n",
    "    \n",
    "    :param block: A list of frames, each frame is a dictionary with 'first_detection_keypoints'.\n",
    "    :param window_size: Size of the rolling window for averaging.\n",
    "    :return: A new block with smoothed keypoints.\n",
    "    \"\"\"\n",
    "    # Convert block to numpy array for easier manipulation\n",
    "    keypoints_array = np.array([frame['first_detection_keypoints'] for frame in block])\n",
    "    num_frames, num_keypoints, _ = keypoints_array.shape\n",
    "    \n",
    "    # Initialize smoothed keypoints array\n",
    "    smoothed_keypoints = np.copy(keypoints_array)\n",
    "    \n",
    "    # Apply rolling average\n",
    "    for i in range(num_frames):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(num_frames, i + window_size // 2 + 1)\n",
    "        smoothed_keypoints[i] = np.mean(keypoints_array[start:end], axis=0)\n",
    "    \n",
    "    # Update block with smoothed keypoints\n",
    "    smoothed_block = []\n",
    "    for i, frame in enumerate(block):\n",
    "        smoothed_frame = frame.copy()\n",
    "        smoothed_frame['first_detection_keypoints'] = smoothed_keypoints[i].tolist()\n",
    "        smoothed_block.append(smoothed_frame)\n",
    "    \n",
    "    return smoothed_block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keypoint_displacements(block):\n",
    "    \"\"\"\n",
    "    Calculate the total displacement for each keypoint in a block.\n",
    "    \n",
    "    :param block: List of frames, each frame is a dictionary with 'first_detection_keypoints'.\n",
    "    :return: A list with the total displacement for each keypoint.\n",
    "    \"\"\"\n",
    "    # Assuming each frame's keypoints are in the same order.\n",
    "    displacements = [0] * len(block[0]['first_detection_keypoints'])  # Initialize displacements\n",
    "    \n",
    "    for i in range(1, len(block)):\n",
    "        prev_keypoints = np.array(block[i-1]['first_detection_keypoints'])\n",
    "        curr_keypoints = np.array(block[i]['first_detection_keypoints'])\n",
    "        distances = np.linalg.norm(curr_keypoints - prev_keypoints, axis=1)\n",
    "        displacements += distances  # Update total displacement for each keypoint\n",
    "\n",
    "    displacements = np.array(displacements) / len(block)  # Normalize by number of frames\n",
    "    \n",
    "    return displacements\n",
    "\n",
    "def filter_blocks_by_displacement(blocks, threshold):\n",
    "    \"\"\"\n",
    "    Filter blocks to keep those where at least one keypoint's total displacement exceeds the threshold.\n",
    "    \n",
    "    :param blocks: List of blocks, each block is a list of frames.\n",
    "    :param threshold: Displacement threshold for filtering.\n",
    "    :return: Filtered list of blocks.\n",
    "    \"\"\"\n",
    "    filtered_blocks = []\n",
    "    \n",
    "    for block in blocks:\n",
    "        displacements = calculate_keypoint_displacements(block)\n",
    "        if np.mean(displacements) > threshold:\n",
    "            filtered_blocks.append(block)\n",
    "    \n",
    "    return filtered_blocks\n",
    "\n",
    "def get_orig_video_info(file):\n",
    "    file_path = file  # change to your own video path\n",
    "\n",
    "    try:\n",
    "        vid = cv2.VideoCapture(file_path)\n",
    "        height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "        width = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "        fps = vid.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        center_x = (width) / 2\n",
    "        center_y = (height) / 2\n",
    "    except cv2.error as e:\n",
    "        print(f\"Caugt cv2 error, setting dummy params\")\n",
    "        width = 0\n",
    "        height = 0\n",
    "        center_x = 0\n",
    "        center_y = 0\n",
    "        fps = 0\n",
    "        \n",
    "    return width, height, center_x, center_y, fps\n",
    "\n",
    "def find_file_by_basename(directory, base_name):\n",
    "    \"\"\"\n",
    "    Find a file in the specified directory that has the given base name with any extension.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): The directory to search in.\n",
    "    - base_name (str): The base name of the file to find.\n",
    "\n",
    "    Returns:\n",
    "    - str: The path of the first matching file found, or None if no match is found.\n",
    "    \"\"\"\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.splitext(filename)[0].lower() == base_name.lower():\n",
    "            return os.path.join(directory, filename)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_keypoints(keypoints, confidence_scores):\n",
    "    \"\"\"\n",
    "    Reorder the keypoints to the OpenPose format.\n",
    "    The OpenPose format is as follows:\n",
    "    0-17: [nose, neck, right_shoulder, right_elbow, right_wrist, left_shoulder, left_elbow, left_wrist,\n",
    "           right_hip, right_knee, right_ankle, left_hip, left_knee, left_ankle, right_eye, left_eye, right_ear, left_ear]\n",
    "    The input 'keypoints' is a list of (x, y, c) tuples, where c is the confidence score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reorder the keypoints to the OpenPose format\n",
    "    keypoints = [keypoints[i] for i in [0, 17, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]]\n",
    "    confidence_scores = [confidence_scores[i] for i in [0, 17, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]]\n",
    "\n",
    "    return keypoints, confidence_scores\n",
    "\n",
    "def rescale_keypoints(keypoints, scale):\n",
    "    \"\"\"\n",
    "    Rescale the keypoints by the given scale.\n",
    "    The input 'keypoints' is a list of (x, y) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    # Rescale the keypoints\n",
    "    keypoints = [(x * scale, y * scale) for (x, y) in keypoints]\n",
    "\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "def convert_coco_to_openpose(coco_keypoints, confidence_scores):\n",
    "    \"\"\"\n",
    "    Convert COCO keypoints to OpenPose keypoints with the neck keypoint as the midpoint between the two shoulders.\n",
    "    COCO keypoints format (17 keypoints): [nose, left_eye, right_eye, left_ear, right_ear,\n",
    "                                           left_shoulder, right_shoulder, left_elbow, right_elbow,\n",
    "                                           left_wrist, right_wrist, left_hip, right_hip,\n",
    "                                           left_knee, right_knee, left_ankle, right_ankle]\n",
    "    OpenPose keypoints format (18 keypoints): COCO keypoints + [neck]\n",
    "    The neck is not a part of COCO keypoints and is computed as the midpoint between the left and right shoulders.\n",
    "    \"\"\"\n",
    "\n",
    "    # Assuming coco_keypoints is a list of (x, y) tuples\n",
    "    nose, left_eye, right_eye, left_ear, right_ear, \\\n",
    "    left_shoulder, right_shoulder, left_elbow, right_elbow, \\\n",
    "    left_wrist, right_wrist, left_hip, right_hip, \\\n",
    "    left_knee, right_knee, left_ankle, right_ankle = coco_keypoints\n",
    "\n",
    "    # Calculate the neck as the midpoint between left_shoulder and right_shoulder\n",
    "    neck_x = (left_shoulder[0] + right_shoulder[0]) / 2\n",
    "    neck_y = (left_shoulder[1] + right_shoulder[1]) / 2\n",
    "    neck = (neck_x, neck_y)\n",
    "\n",
    "    # Assuming coco_keypoints is a list of (x, y) tuples\n",
    "    c_nose, c_left_eye, c_right_eye, c_left_ear, c_right_ear, \\\n",
    "    c_left_shoulder, c_right_shoulder, c_left_elbow, c_right_elbow, \\\n",
    "    c_left_wrist, c_right_wrist, c_left_hip, c_right_hip, \\\n",
    "    c_left_knee, c_right_knee, c_left_ankle, c_right_ankle = confidence_scores\n",
    "\n",
    "    # Calculate the neck as the midpoint between left_shoulder and right_shoulder\n",
    "    c_neck = (c_left_shoulder + c_right_shoulder) / 2\n",
    "\n",
    "    # Construct the OpenPose keypoints including the neck\n",
    "    openpose_keypoints = [\n",
    "        nose, left_eye, right_eye, left_ear, right_ear,\n",
    "        left_shoulder, right_shoulder, left_elbow, right_elbow,\n",
    "        left_wrist, right_wrist, left_hip, right_hip,\n",
    "        left_knee, right_knee, left_ankle, right_ankle,\n",
    "        neck  # Adding the neck as the last keypoint\n",
    "    ]\n",
    "    \n",
    "    openpose_confidences = [\n",
    "        c_nose, c_left_eye, c_right_eye, c_left_ear, c_right_ear,\n",
    "        c_left_shoulder, c_right_shoulder, c_left_elbow, c_right_elbow,\n",
    "        c_left_wrist, c_right_wrist, c_left_hip, c_right_hip,\n",
    "        c_left_knee, c_right_knee, c_left_ankle, c_right_ankle,\n",
    "        c_neck  # Adding the neck as the last keypoint\n",
    "    ]\n",
    "\n",
    "    openpose_keypoints, confidences = reorder_keypoints(openpose_keypoints, openpose_confidences)\n",
    "    openpose_keypoints = rescale_keypoints(openpose_keypoints, 1)\n",
    "\n",
    "    return openpose_keypoints, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fps(videoname):\n",
    "    clip = VideoFileClip(videoname)\n",
    "    return clip.fps\n",
    "\n",
    "def read_video(video):\n",
    "\n",
    "    videogen = skvideo.io.vreader(video)\n",
    "    new_videogen = itertools.islice(videogen, 0, 1, 1)\n",
    "    for image in new_videogen:\n",
    "        a = 1\n",
    "    return image\n",
    "\n",
    "def get_video_information_yt(file_path):\n",
    "    videofiles = np.array(glob.glob(os.path.join(file_path,'video*')))\n",
    "    videofiles = videofiles[np.array([len(os.path.basename(i)) if i[-3:]!='pkl' else 0 for i in videofiles])==len('video_000000.mp4')]\n",
    "    # get fps and screen dim\n",
    "    df_fps = pd.DataFrame()\n",
    "    fpsl = []\n",
    "    rowlist = []; collist = []\n",
    "    for ivideo in videofiles:\n",
    "        print(ivideo)\n",
    "        if os.path.basename(ivideo)[-3:]=='avi':\n",
    "            fps = 30\n",
    "        else:\n",
    "            fps = get_fps(ivideo)\n",
    "        fpsl.append(fps)\n",
    "        img = read_video(ivideo)\n",
    "        nrows = len(img)\n",
    "        ncols = len(img[0])\n",
    "        rowlist.append(nrows); collist.append(ncols);\n",
    "    df_fps['fps'] = pd.Series(fpsl)\n",
    "    df_fps['pixel_x'] = pd.Series(collist)\n",
    "    df_fps['pixel_y'] = pd.Series(rowlist)\n",
    "    df_fps['video'] = [os.path.basename(i)[:-4] for i in videofiles]\n",
    "    return df_fps\n",
    "\n",
    "def get_video_information_clinical(file_path):\n",
    "    videofiles = np.array(glob.glob(os.path.join(file_path,'822487*')))\n",
    "    videofiles = [i for i in videofiles if i[-3:]!='pkl']\n",
    "    videofiles = [i for i in videofiles if i[-len('openposeLabeled.mp4'):]!='openposeLabeled.mp4']\n",
    "    # get fps and screen dim\n",
    "    df_fps = pd.DataFrame()\n",
    "    fpsl = []\n",
    "    rowlist = []; collist = []\n",
    "    for ivideo in videofiles:\n",
    "        if os.path.basename(ivideo)[:6]=='822487':\n",
    "            fps = 30\n",
    "        else:\n",
    "            fps = get_fps(ivideo)\n",
    "        fpsl.append(fps)\n",
    "        img = read_video(ivideo)\n",
    "        nrows = len(img)\n",
    "        ncols = len(img[0])\n",
    "        rowlist.append(nrows); collist.append(ncols);\n",
    "    df_fps['fps'] = pd.Series(fpsl)\n",
    "    df_fps['pixel_x'] = pd.Series(collist)\n",
    "    df_fps['pixel_y'] = pd.Series(rowlist)\n",
    "    df_fps['video'] = [os.path.basename(i)[:-4] for i in videofiles]\n",
    "    return df_fps\n",
    "\n",
    "def load_raw_pkl_files(path):\n",
    "    pklfiles = np.array(glob.glob(os.path.join(path,'*.pkl')))\n",
    "    df_pkl = pd.DataFrame()\n",
    "    for file in pklfiles:\n",
    "        one_file = pd.read_pickle(file).reset_index().drop('index',axis = 1)\n",
    "        df_pkl = pd.concat([df_pkl, one_file])\n",
    "    df_pkl = df_pkl.reset_index().drop('index', axis = 1)\n",
    "    return df_pkl\n",
    "\n",
    "def get_skel(df):\n",
    "    if len(list(itertools.chain(*df.limbs_subset)))>0:\n",
    "        peaks = df.peaks.iloc[0]\n",
    "        parts_in_skel = df.limbs_subset.iloc[0]\n",
    "        person_to_peak_mapping = [list(i[:-2]) for i in parts_in_skel] \n",
    "        skel_idx = [[i]*(len(iskel)-2) for i, iskel in enumerate(parts_in_skel)]\n",
    "        idx_df = pd.DataFrame.from_dict({'peak_idx':list(itertools.chain(*person_to_peak_mapping)),\\\n",
    "         'person_idx':list(itertools.chain(*skel_idx))})\n",
    "        peaks_list = list(chain.from_iterable(peaks))\n",
    "        x = [ipeak[0] for ipeak in peaks_list]\n",
    "        y = [ipeak[1] for ipeak in peaks_list]\n",
    "        c = [ipeak[2] for ipeak in peaks_list]\n",
    "        peak_idx = [ipeak[3] for ipeak in peaks_list]\n",
    "        kp_idx = list(chain.from_iterable([len(ipeak)*[i] for i,ipeak in enumerate(peaks)]))\n",
    "        peak_df = pd.DataFrame.from_dict({'x':x,'y':y,'c':c,'peak_idx':peak_idx,'part_idx':kp_idx})\n",
    "        kp_df = pd.merge(idx_df, peak_df, on='peak_idx', how='left').drop('peak_idx',axis=1)\n",
    "        kp_df = kp_df.loc[~kp_df.c.isnull(),:]\n",
    "    else:\n",
    "        kp_df = pd.DataFrame()\n",
    "    return kp_df\n",
    "\n",
    "def edit_df(df, df_fps):\n",
    "    # keep person index with max number of keypoints per frame\n",
    "    counts = df.groupby(['video','frame', 'person_idx'])['c'].count().reset_index()\n",
    "    max_rows = counts.groupby(['video','frame'])['c'].idxmax().tolist()\n",
    "    max_rows_df = counts.loc[max_rows,['video','frame', 'person_idx']]\n",
    "    max_rows_df['dum'] = 1\n",
    "    df = pd.merge(df.reset_index(), max_rows_df, on=['video','frame', 'person_idx'], how='inner')\n",
    "\n",
    "    # add keypoint labels\n",
    "    bps = [\"Nose\",\"Neck\",\"RShoulder\",\"RElbow\",\"RWrist\",\"LShoulder\",\"LElbow\",\"LWrist\",\"RHip\",\"RKnee\",\"RAnkle\",\"LHip\",\"LKnee\",\\\n",
    "     \"LAnkle\",\"REye\",\"LEye\",\"REar\",\"LEar\"]\n",
    "    df['bp'] = [bps[int(i)] for i in df.part_idx]\n",
    "    df = df[['video','frame', 'x', 'y', 'bp', 'part_idx']] \n",
    "\n",
    "    # include row for each keypoint and frame\n",
    "    max_frame = df.groupby('video').frame.max().reset_index()\n",
    "    max_frame['frame_vec'] = max_frame.frame.apply(lambda x: np.arange(0,x+1))\n",
    "    max_frame['bp'] = pd.Series([bps]*len(max_frame))\n",
    "    y =[]\n",
    "    _ = max_frame.apply(lambda x: [y.append([x['video'], x['bp'], i]) for i in x.frame_vec], axis=1)\n",
    "    all_frames = pd.DataFrame(y, columns = ['video','bp','frame'])\n",
    "    z =[]\n",
    "    _ = all_frames.apply(lambda x: [z.append([x['video'], x['frame'], i]) for i in x.bp], axis=1)\n",
    "    all_frames = pd.DataFrame(z, columns = ['video','frame', 'bp'])\n",
    "    df = pd.merge(df, all_frames, on = ['video','frame','bp'], how='outer')\n",
    "    df = pd.merge(df,df_fps, on = 'video', how='outer')\n",
    "    df['time'] = df['frame']/df['fps']\n",
    "    \n",
    "    part_idx_df = df[['bp', 'part_idx']].drop_duplicates().dropna().sort_values('part_idx')\n",
    "    df = pd.merge(df.drop('part_idx', axis=1), part_idx_df, on= 'bp', how='inner')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def interpolate_df(df):\n",
    "    df = df.sort_values('frame')\n",
    "    df['x']=df.x.interpolate()\n",
    "    df['y']=df.y.interpolate()\n",
    "    return df\n",
    "\n",
    "def smooth(d, var, winmed, winmean):\n",
    "    winmed1 = winmed\n",
    "    winmed = int(winmed*d.fps.unique()[0])\n",
    "    winmean = int(winmean*d.fps.unique()[0])\n",
    "    d = d.reset_index(drop=True)\n",
    "    if winmed>0:\n",
    "        x = d.sort_values('frame')[var].rolling(center=True,window=winmed).median()\n",
    "        d[var] = x.rolling(center=True,window=winmean).mean()\n",
    "    else:\n",
    "        d[var] = d[var]\n",
    "    d['smooth'] = winmed1\n",
    "    return d\n",
    "\n",
    "def comp_joint_angle(df, joint_str):\n",
    "    df = df.loc[(df.bp=='L'+joint_str)|(df.bp=='R'+joint_str)]\n",
    "    df = pd.pivot_table(df, columns = ['bp'], values=['x', 'y'], index=['frame'])\n",
    "    # zangle =np.arctan2(Rj.y.iloc[0]-Lj.y.iloc[0],Rj.x.iloc[0]-Lj.x.iloc[0])\n",
    "    df[joint_str+'_angle']= np.arctan2((df['y', 'R'+joint_str]-df['y', 'L'+joint_str]),(df['x', 'R'+joint_str]-df['x', 'L'+joint_str]))\n",
    "    df = df.drop(['x', 'y'], axis=1)\n",
    "    return df\n",
    "\n",
    "def comp_center_joints(df, joint_str, jstr):\n",
    "    df = df.loc[(df.bp=='L'+joint_str)|(df.bp=='R'+joint_str)]\n",
    "    df = pd.pivot_table(df, columns = ['bp'], values=['x', 'y'], index=['frame'])\n",
    "    # zangle =np.arctan2(Rj.y.iloc[0]-Lj.y.iloc[0],Rj.x.iloc[0]-Lj.x.iloc[0])\n",
    "    df[jstr+'y']= (df['y', 'R'+joint_str]+df['y', 'L'+joint_str])/2\n",
    "    df[jstr+'x']= (df['x', 'R'+joint_str]+df['x', 'L'+joint_str])/2\n",
    "    df = df.drop(['x', 'y'], axis=1)\n",
    "    return df\n",
    "\n",
    "def normalise_skeletons(df):\n",
    "    \n",
    "    ''' Rotate keypoints around reference points (center of shoulders, center of hips)\\\n",
    "    Normalise points by reference distance (trunk length)'''\n",
    "\n",
    "    bps = [\"Nose\",\"Neck\",\"RShoulder\",\"RElbow\",\"RWrist\",\"LShoulder\",\"LElbow\",\"LWrist\",\"RHip\",\"RKnee\",\"RAnkle\",\"LHip\",\"LKnee\",\\\n",
    "     \"LAnkle\",\"REye\",\"LEye\",\"REar\",\"LEar\"]\n",
    "    ubps = [\"Nose\",\"Neck\",\"RShoulder\",\"RElbow\",\"RWrist\",\"LShoulder\",\"LElbow\",\"LWrist\", \"REye\",\"LEye\",\"REar\",\"LEar\"]\n",
    "    lbps = [\"RKnee\",\"RAnkle\",\"LHip\",\"LKnee\",\"LAnkle\",\"RHip\"]\n",
    "    u_idx = np.where(np.isin(bps, ubps)==1)[0]\n",
    "    l_idx = np.where(np.isin(bps, lbps)==1)[0]\n",
    "    df['upper'] = np.isin(df.bp, ubps)*1\n",
    "    # compute shoulder and hip angles for rotating, upper and lower body \n",
    "    # reference parts, now center of shoulders and hips\n",
    "    \n",
    "    s_angle = df.groupby(['video']).apply(lambda x: comp_joint_angle(x,'Shoulder')).reset_index()\n",
    "    h_angle = df.groupby(['video']).apply(lambda x: comp_joint_angle(x,'Hip')).reset_index()\n",
    "    uref = df.groupby(['video']).apply(lambda x: comp_center_joints(x, 'Shoulder', 'uref')).reset_index()\n",
    "    lref = df.groupby(['video']).apply(lambda x: comp_center_joints(x, 'Hip', 'lref')).reset_index()\n",
    "    s_angle['Hip_angle'] = h_angle['Hip_angle']\n",
    "    s_angle = pd.merge(s_angle, uref, on=['video', 'frame'], how='inner')\n",
    "    s_angle = pd.merge(s_angle, lref, on=['video', 'frame'], how='inner')\n",
    "    s_angle.columns = s_angle.columns.get_level_values(0)\n",
    "\n",
    "    df = pd.merge(df,s_angle, on=['video', 'frame'], how = 'outer')\n",
    "    # set up columns, reference parts and reference angles\n",
    "\n",
    "    df['refx'] = df['urefx']*df['upper'] + df['lrefx']*(1-df['upper'])\n",
    "    df['refy'] = df['urefy']*df['upper'] + df['lrefy']*(1-df['upper'])\n",
    "    df['ref_dist'] = np.sqrt((df['urefy']-df['lrefy'])**2+(df['urefx']-df['lrefx'])**2)\n",
    "    df['ref_angle'] = df['Shoulder_angle']*df['upper'] + df['Hip_angle']*(1-df['upper'])\n",
    "\n",
    "    df.loc[df.ref_angle<0,'ref_angle'] = 2*np.pi + df.loc[df.ref_angle<0,'ref_angle'] \n",
    "    df.loc[df.ref_angle<np.pi,'ref_angle'] = np.pi - df.loc[df.ref_angle<np.pi,'ref_angle'] \n",
    "    df.loc[(df.ref_angle>np.pi)&(df.ref_angle<2*np.pi),'ref_angle'] = 3*np.pi - df.loc[(df.ref_angle>np.pi)&(df.ref_angle<2*np.pi),'ref_angle']\n",
    "    df['x_rotate'] = df['refx'] + np.cos(df['ref_angle'])*(df['x']-df['refx']) - np.sin(df['ref_angle'])*(df['y'] - df['refy'])\n",
    "    df['y_rotate'] = df['refy'] + np.sin(df['ref_angle'])*(df['x']-df['refx']) + np.cos(df['ref_angle'])*(df['y'] - df['refy'])\n",
    "    df['x_rotate'] = (df['x_rotate']-df['refx'])/df['ref_dist']\n",
    "    df['y_rotate'] = (df['y_rotate']-df['refy'])/df['ref_dist']\n",
    "    df['x'] = df['x_rotate']\n",
    "    df['y'] = df['y_rotate']\n",
    "    # add to lower body to make trunk length 1\n",
    "    df.loc[df.upper==0,'y'] = df.loc[df.upper==0,'y']+1\n",
    "    df['delta_t'] = 1/df['fps']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_joint_angles(df):\n",
    "    \n",
    "    ''' Compute joint angles from x,y coordinates '''\n",
    "\n",
    "    df = df[~df.x.isnull()]\n",
    "    df['delta_t'] = 1/df['fps']\n",
    "\n",
    "    bps = [\"Nose\",\"Neck\",\"RShoulder\",\"RElbow\",\"RWrist\",\"LShoulder\",\"LElbow\",\"LWrist\",\"RHip\",\"RKnee\",\"RAnkle\",\"LHip\",\"LKnee\",\\\n",
    "     \"LAnkle\",\"REye\",\"LEye\",\"REar\",\"LEar\"]\n",
    "    joints = [['part0', 'part1', 'part2'],\\\n",
    "    ['RShoulder','Neck', 'RElbow'],\\\n",
    "    ['RElbow', 'RShoulder','RWrist'],\\\n",
    "    ['RHip','LHip', 'RKnee'],\\\n",
    "    ['RKnee', 'RHip','RAnkle'],\\\n",
    "    ['LShoulder','Neck', 'LElbow'],\\\n",
    "    ['LElbow', 'LShoulder','LWrist'],\\\n",
    "    ['LHip','RHip', 'LKnee'],\\\n",
    "    ['LKnee', 'LHip','LAnkle']]\n",
    "    headers = joints.pop(0)\n",
    "    df_joints = pd.DataFrame(joints, columns=headers).reset_index()\n",
    "    df_joints['bpindex'] = df_joints['index']+1\n",
    "\n",
    "    df_1 = df\n",
    "    for i in [0,1,2]:\n",
    "        df_joints['bp'] = df_joints['part'+str(i)]\n",
    "        df_1 = pd.merge(df_1,df_joints[['bp', 'bpindex']], on='bp', how='left')\n",
    "        df_1['idx'+str(i)] = df_1['bpindex'] \n",
    "        df_1 = df_1.drop('bpindex', axis=1)\n",
    "        df_1['x'+str(i)] = df_1['x']*df_1['idx'+str(i)]/df_1['idx'+str(i)]\n",
    "        df_1['y'+str(i)] = df_1['y']*df_1['idx'+str(i)]/df_1['idx'+str(i)]\n",
    "    df0 = df_1[['video', 'frame', 'idx0', 'x0', 'y0', 'bp']]; df0 = df0.rename(index=str, columns={\"bp\": \"bp0\", \"idx0\": \"idx\"}); df0 = df0[~df0.idx.isnull()]\n",
    "    df1 = df_1[['video', 'frame', 'idx1', 'x1', 'y1', 'bp']]; df1 = df1.rename(index=str, columns={\"bp\": \"bp1\", \"idx1\": \"idx\"}); df1 = df1[~df1.idx.isnull()]\n",
    "    df2 = df_1[['video', 'frame', 'idx2', 'x2', 'y2', 'bp']]; df2 = df2.rename(index=str, columns={\"bp\": \"bp2\", \"idx2\": \"idx\"}); df2 = df2[~df2.idx.isnull()]\n",
    "    df_2 = pd.merge(df0,df1, on=['video', 'frame', 'idx'], how='inner')\n",
    "    df_2 = pd.merge(df_2,df2, on=['video', 'frame', 'idx'], how='inner')\n",
    "\n",
    "    # compute angle here\n",
    "    df_2['dot'] = (df_2['x1'] - df_2['x0'])*(df_2['x2'] - df_2['x0']) + (df_2['y1'] - df_2['y0'])*(df_2['y2'] - df_2['y0'])\n",
    "    df_2['det'] = (df_2['x1'] - df_2['x0'])*(df_2['y2'] - df_2['y0']) - (df_2['y1'] - df_2['y0'])*(df_2['x2'] - df_2['x0'])\n",
    "    df_2['angle_degs'] = np.arctan2(df_2['det'],df_2['dot'])*180/np.pi\n",
    "    # hip and shoulder should be same regardless of side\n",
    "    # elbow/knee give flexion/extension information only\n",
    "    df_2['side'] = df_2.bp0.str[:1]\n",
    "    df_2['part'] = df_2.bp0.str[1:]\n",
    "    df_2['angle'] = df_2.angle_degs # same on left/right\n",
    "    df_2.loc[(df_2['bp0']=='LShoulder')|(df_2['bp0']=='LHip'),'angle'] = \\\n",
    "    df_2.loc[(df_2['bp0']=='LShoulder')|(df_2['bp0']=='LHip'),'angle']*(-1)\n",
    "    df_2.loc[(df_2['part']=='Elbow')|(df_2['part']=='Knee'),'angle'] = \\\n",
    "    np.abs(df_2.loc[(df_2['part']=='Elbow')|(df_2['part']=='Knee'),'angle'])\n",
    "    # shoulders/hips: change to -180-+180 to 0-360 if neg: 360+angle\n",
    "    df_2.loc[((df_2['part']=='Shoulder')|(df_2['part']=='Hip'))& (df_2.angle<0),'angle'] = \\\n",
    "    df_2.loc[((df_2['part']=='Shoulder')|(df_2['part']=='Hip'))& (df_2.angle<0),'angle']+360\n",
    "    # can include shoulder rotation\n",
    "    df_2['bp'] = df_2['bp0']\n",
    "\n",
    "    df_info = df.groupby(['video', 'frame', 'fps','time', 'delta_t']).mean(numeric_only=True).reset_index()[['video', 'frame', 'fps','time', 'delta_t']]\n",
    "    df_angle = pd.merge(df_2[['video', 'frame', 'bp', 'side', 'part', 'angle']],\\\n",
    "    df_info, on=['video', 'frame'], how='inner').drop_duplicates()\n",
    "    return df_angle\n",
    "\n",
    "\n",
    "def angular_disp(x,y): \n",
    "    possible_angles = np.asarray([y-x, y-x+360, y-x-360])\n",
    "    idxMinAbsAngle = np.abs([y-x, y-x+360, y-x-360]).argmin(axis=0)\n",
    "    smallest_angle = np.asarray([possible_angles[idxMinAbsAngle[i],i] for i in range(len(possible_angles[0]))])\n",
    "    return smallest_angle\n",
    "\n",
    "def get_angle_displacement(df, inp, outp): # different from other deltas - need shortest path\n",
    "    df = df.sort_values('frame')\n",
    "    angle = np.array(df[inp])\n",
    "    a = angular_disp(angle[0:len(angle)-1], angle[1:len(angle)])\n",
    "    df[outp] = np.concatenate((np.asarray([0]),a))\n",
    "    return df\n",
    "\n",
    "def smooth_dyn(df, inp, outp, win):\n",
    "    fps = df['fps'].unique()[0]\n",
    "    win = int(win*fps)\n",
    "    x = df[inp].interpolate()\n",
    "    df[outp] = x.rolling(window=win,center=False).mean()\n",
    "    return df\n",
    "\n",
    "def get_delta(df, inp, outp):\n",
    "    x = df[inp]\n",
    "    df[outp]  = np.concatenate((np.asarray([0]),np.diff(x)))*(np.asarray(x*0)+1)\n",
    "    return df\n",
    "\n",
    "def get_dynamics_xy(xdf, delta_window):\n",
    "    # get velocity, acceleration\n",
    "    xdf = xdf[['video','frame','x','y','bp','fps','pixel_x', 'pixel_y','time','delta_t', 'part_idx']]\n",
    "    xdf = xdf.groupby(['bp','video']).apply(lambda x: get_delta(x,'x','d_x')).reset_index(drop=True)\n",
    "    xdf = xdf.groupby(['bp','video']).apply(lambda x: get_delta(x,'y','d_y')).reset_index(drop=True)\n",
    "    xdf['displacement'] = np.sqrt(xdf['d_x']**2 + xdf['d_y']**2)\n",
    "    xdf['velocity_x_raw'] = xdf['d_x']/xdf['delta_t']\n",
    "    xdf['velocity_y_raw'] = xdf['d_y']/xdf['delta_t']\n",
    "    xdf = xdf.groupby(['bp','video']).apply(lambda x: smooth_dyn(x,'velocity_x_raw','velocity_x', delta_window)).reset_index(drop=True)\n",
    "    xdf = xdf.groupby(['bp','video']).apply(lambda x: smooth_dyn(x,'velocity_y_raw','velocity_y', delta_window)).reset_index(drop=True)\n",
    "    xdf = xdf.groupby(['bp','video']).apply(lambda x: get_delta(x,'velocity_x','delta_velocity_x')).reset_index(drop=True)\n",
    "    xdf = xdf.groupby(['bp','video']).apply(lambda x: get_delta(x,'velocity_y','delta_velocity_y')).reset_index(drop=True)\n",
    "    xdf['acceleration_x_raw'] = xdf['delta_velocity_x']/xdf['delta_t']\n",
    "    xdf['acceleration_y_raw'] = xdf['delta_velocity_y']/xdf['delta_t']\n",
    "    xdf = xdf.groupby(['bp','video']).apply(lambda x: smooth_dyn(x,'acceleration_x_raw','acceleration_x', delta_window)).reset_index(drop=True)\n",
    "    xdf = xdf.groupby(['bp','video']).apply(lambda x: smooth_dyn(x,'acceleration_y_raw','acceleration_y', delta_window)).reset_index(drop=True)\n",
    "    xdf['acceleration_x2'] = xdf['acceleration_x']**2\n",
    "    xdf['acceleration_y2'] = xdf['acceleration_y']**2\n",
    "    xdf['speed_raw'] = xdf['displacement']/xdf['delta_t']\n",
    "    xdf = xdf.groupby(['bp','video']).apply(lambda x: smooth_dyn(x,'speed_raw','speed', delta_window)).reset_index(drop=True)\n",
    "    xdf['part'] = xdf.bp.str[1:]\n",
    "    xdf['side'] = xdf.bp.str[:1]\n",
    "    return xdf\n",
    "\n",
    "def get_dynamics_angle(adf, delta_window):\n",
    "    adf = adf.groupby(['bp','video']).apply(lambda x: get_angle_displacement(x,'angle','displacement')).reset_index(drop=True)\n",
    "    adf['velocity_raw'] = adf['displacement']/adf['delta_t']\n",
    "    adf = adf.groupby(['bp','video']).apply(lambda x: smooth_dyn(x,'velocity_raw','velocity', delta_window)).reset_index(drop=True)\n",
    "    adf = adf.groupby(['bp','video']).apply(lambda x: get_delta(x,'velocity','delta_velocity')).reset_index(drop=True)\n",
    "    adf['acceleration_raw'] = adf['delta_velocity']/adf['delta_t']\n",
    "    adf = adf.groupby(['bp','video']).apply(lambda x: smooth_dyn(x,'acceleration_raw','acceleration', delta_window)).reset_index(drop=True)\n",
    "    adf['acceleration2'] = adf['acceleration']**2\n",
    "    adf['part'] = adf.bp.str[1:]\n",
    "    adf['side'] = adf.bp.str[:1]\n",
    "    return adf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_features(df):\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    # - absolute angle\n",
    "    a_mean = np.degrees(CS.nanmean(np.array(np.radians(df['angle']))))\n",
    "    # - variability of angle\n",
    "    a_stdev = np.sqrt(np.degrees(CS.nanvar(np.array(np.radians(df['angle'])))))\n",
    "    # - measure of complexity (entropy)\n",
    "    a_ent = ent(df['angle'].round())\n",
    "    # - median absolute velocity\n",
    "    median_vel = (np.abs(df['velocity'])).median()\n",
    "    # - variability of velocity\n",
    "    IQR_vel = (df['velocity']).quantile(.75) - (df['velocity']).quantile(.25)\n",
    "    # - variability of acceleration\n",
    "    IQR_acc = df['acceleration'].quantile(.75) - df['acceleration'].quantile(.25)\n",
    "\n",
    "    return pd.DataFrame.from_dict({'video':np.unique(df.video),'bp':np.unique(df.bp),\\\n",
    "    'mean_angle':a_mean, 'stdev_angle':a_stdev, 'entropy_angle':a_ent,\n",
    "    'median_vel_angle':median_vel,'IQR_vel_angle':IQR_vel,\\\n",
    "    'IQR_acc_angle': IQR_acc})\n",
    "\n",
    "def xy_features(df):\n",
    "    # - absolute position/angle    \n",
    "    median_x = df['x'].median()\n",
    "    median_y = df['y'].median()\n",
    "    IQR_x = df['x'].quantile(.75)-df['x'].quantile(.25)\n",
    "    IQR_y = df['y'].quantile(.75)-df['y'].quantile(.25)\n",
    "    # - median absolute velocity\n",
    "    median_vel_x = np.abs(df['velocity_x']).median()\n",
    "    median_vel_y = np.abs(df['velocity_y']).median()\n",
    "    # - variability of velocity\n",
    "    IQR_vel_x = df['velocity_x'].quantile(.75)-df['velocity_x'].quantile(.25)\n",
    "    IQR_vel_y = df['velocity_y'].quantile(.75)-df['velocity_y'].quantile(.25)\n",
    "    \n",
    "    # - variability of acceleration\n",
    "    IQR_acc_x = df['acceleration_x'].quantile(.75) - df['acceleration_x'].quantile(.25)\n",
    "    IQR_acc_y = df['acceleration_y'].quantile(.75) - df['acceleration_y'].quantile(.25)\n",
    "    \n",
    "    # - measure of complexity (entropy)\n",
    "    ent_x = ent(df['x'].round(2))\n",
    "    ent_y = ent(df['y'].round(2))\n",
    "    mean_ent = (ent_x+ent_y)/2\n",
    "    # define part and side here\n",
    "    return pd.DataFrame.from_dict({'video':np.unique(df.video),'bp':np.unique(df.bp),\\\n",
    "    'medianx': median_x, 'mediany': median_y, 'IQRx': IQR_x,'IQRy': IQR_y,\\\n",
    "    'medianvelx':median_vel_x, 'medianvely':median_vel_y,\\\n",
    "    'IQRvelx':IQR_vel_x,'IQRvely':IQR_vel_y,\\\n",
    "    'IQRaccx':IQR_acc_x,'IQRaccy':IQR_acc_y,'meanent':mean_ent})\n",
    "\n",
    "def ent(data):\n",
    "    p_data= data.value_counts()/len(data) #  probabilities\n",
    "    entropy=sc.stats.entropy(p_data)\n",
    "    return entropy\n",
    "\n",
    "def corr_lr(df, var):\n",
    "    idf = pd.DataFrame()\n",
    "    idf['R'] = df[df.side=='R'].reset_index()[var]\n",
    "    idf['L'] = df[df.side=='L'].reset_index()[var]\n",
    "    return idf.corr().loc['L','R']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find good blocks\n",
    "\n",
    "# Replace 'your_file_path.json' with the actual path to your JSON file\n",
    "file_path = '../outputs/PANDA1/openpose/annotations/'\n",
    "vis_path = '../outputs/PANDA1/openpose/vis/'\n",
    "\n",
    "for file in os.listdir(file_path):\n",
    "    base_name = os.path.basename(file).split('.')[0]\n",
    "\n",
    "    analysis_results = analyze_file(os.path.join(file_path,file), 0.7)\n",
    "    good_blocks = find_continuous_good_blocks(analysis_results)\n",
    "    good_blocks = [smooth_keypoints(block) for block in good_blocks]\n",
    "\n",
    "    try:\n",
    "        print(f'Found {len(good_blocks)} good blocks. The longest one has {max(len(block) for block in good_blocks)} frames.')\n",
    "            # Calculate the total displacement for each block\n",
    "        displacements = [calculate_keypoint_displacements(block) for block in good_blocks]\n",
    "        movement_blocks = filter_blocks_by_displacement(good_blocks, 1)\n",
    "\n",
    "        # Get video info\n",
    "        width, height, center_x, center_y, fps = get_orig_video_info(f'{os.path.join(vis_path, base_name)}.mp4')\n",
    "        #print(f'total displacements for each block: {np.mean(displacements[0]), np.mean(displacements[1])}')\n",
    "        print(f'number of blocks with movement: {len(movement_blocks)}')    \n",
    "\n",
    "    except ValueError:\n",
    "        print(f'No good blocks found')\n",
    "\n",
    "\n",
    "    print(width, height, fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find good blocks for all listed datasets\n",
    "\n",
    "datasets = ['CLIN','YT', 'PANDA1','PANDA2', 'PANDA3']\n",
    "\n",
    "kp_thr = 0.7\n",
    "data_for_df = []\n",
    "\n",
    "for dataset in datasets:\n",
    "\n",
    "    directory = f'../outputs/{dataset}/openpose'\n",
    "    print(f'Processing dataset: {dataset}')\n",
    "\n",
    "    for file in os.listdir(f'{directory}/annotations'):\n",
    "        # Replace 'your_file_path.json' with the actual path to your JSON file\n",
    "        base_name = os.path.basename(file).split('.')[0]\n",
    "\n",
    "        file_path = f'{directory}/annotations/{base_name}.json'\n",
    "        vis_path = find_file_by_basename(f'{directory}/vis', base_name)\n",
    "\n",
    "        # Get video info\n",
    "        try:\n",
    "            width, height, center_x, center_y, fps = get_orig_video_info(vis_path)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if fps == 0:\n",
    "            continue\n",
    "\n",
    "        analysis_results = analyze_file(file_path, kp_thr)\n",
    "        good_blocks = find_continuous_good_blocks(analysis_results)\n",
    "        good_blocks = [smooth_keypoints(block) for block in good_blocks]\n",
    "\n",
    "        # Calculate the total displacement for each block\n",
    "        #displacements = [calculate_keypoint_displacements(block) for block in good_blocks]\n",
    "        #movement_blocks = filter_blocks_by_displacement(good_blocks, 1)\n",
    "\n",
    "\n",
    "\n",
    "        # Loop over all frames in good_blocks\n",
    "        for e, block in enumerate(good_blocks):\n",
    "\n",
    "            n_frames = len(block)\n",
    "            duration = float(n_frames / fps)\n",
    "\n",
    "            for frame in block:\n",
    "                # Extract keypoints and confidences\n",
    "                keypoints = frame['first_detection_keypoints']\n",
    "                confidences = frame['first_detection_confidence']\n",
    "                \n",
    "                # Extract other frame details\n",
    "                frame_id = frame['frame_id']\n",
    "                part_idx = 0\n",
    "\n",
    "\n",
    "                \n",
    "                # Loop through each keypoint to create a row in the DataFrame\n",
    "                for kp, confidence in zip(keypoints, confidences):\n",
    "                    x_coord, y_coord = kp\n",
    "                    \n",
    "                    data_for_df.append({\n",
    "                        'dataset': dataset,\n",
    "                        'video': base_name,\n",
    "                        'block': e,\n",
    "                        'frame': frame_id,\n",
    "                        'x': x_coord,\n",
    "                        'y': y_coord,\n",
    "                        'c': confidence,\n",
    "                        'part_idx': part_idx,\n",
    "                        'in_skel': 1,\n",
    "                        'include': 1,\n",
    "                        'duration': duration,\n",
    "                        'fps': fps,\n",
    "                        'pixel_x': width,\n",
    "                        'pixel_y': height,\n",
    "                        'time': frame_id / fps\n",
    "                    })\n",
    "\n",
    "                    part_idx += 1\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data_for_df)\n",
    "\n",
    "# Now df contains all the data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Openpose format pose pkl files for each dataset\n",
    "\n",
    "OVERWRITE = True\n",
    "\n",
    "dataset = 'PANDA2B'\n",
    "json_path = f'../outputs/{dataset}/openpose/annotations'\n",
    "json_files = os.listdir(json_path)\n",
    "directory = f'../outputs/{dataset}/openpose'\n",
    "\n",
    "save_path = f'../pose_estimates/{dataset}'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "kp_mapping = {0:'Nose', 1:'Neck', 2:'RShoulder', 3:'RElbow', 4:'RWrist', 5:'LShoulder', 6:'LElbow', \n",
    "              7:'LWrist', 8:'RHip', 9:'RKnee', 10:'RAnkle', 11:'LHip', \n",
    "              12:'LKnee', 13:'LAnkle', 14:'REye', 15:'LEye', 16:'REar', 17:'LEar'}\n",
    "\n",
    "# Define the DataFrame columns as specified\n",
    "columns = ['video_number', 'video', 'bp', 'frame', 'x', 'y', 'c','fps', 'pixel_x', 'pixel_y', 'time', 'part_idx']\n",
    "data = []  # This will hold the data to be loaded into the DataFrame\n",
    "#vid_info = pd.read_csv('../data/vid_info.csv')\n",
    "\n",
    "for file_number, file in enumerate(json_files, start=0):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(json_path, file)\n",
    "    fname = file.split('.')[0]\n",
    "\n",
    "    vis_path = find_file_by_basename(f'{directory}/vis', fname)\n",
    "    print(vis_path)\n",
    "    # Get video info\n",
    "    try:\n",
    "        width, height, center_x, center_y, fps = get_orig_video_info(vis_path)\n",
    "        fps = fps\n",
    "        pixel_x = width\n",
    "        pixel_y = height\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    if fps == 0:\n",
    "        continue\n",
    "    \n",
    "    interim = []\n",
    "\n",
    "    if os.path.exists(f'{save_path}/{fname}.pkl') and not OVERWRITE:\n",
    "        continue\n",
    "    \n",
    "    # Open and load the JSON data\n",
    "    with open(file_path, 'r') as f:\n",
    "        frames = json.load(f)\n",
    "        \n",
    "        # Iterate through each frame in the JSON file\n",
    "        for frame in frames:\n",
    "            frame_id = frame['frame_id']\n",
    "            if 'instances' in frame and len(frame['instances']) > 0:\n",
    "                keypoints = frame['instances'][0]['keypoints']\n",
    "                confidence = frame['instances'][0]['keypoint_scores']\n",
    "                keypoints, confidence = convert_coco_to_openpose(keypoints, confidence)\n",
    "\n",
    "                # Iterate through each keypoint\n",
    "                for part_idx, (x, y) in enumerate(keypoints):\n",
    "                    # Assuming you have a way to map part_idx to a body part name 'bp'\n",
    "\n",
    "                    bp = kp_mapping[part_idx]  \n",
    "                    fps = fps  \n",
    "                    time = frame_id / fps\n",
    "                    c = confidence[part_idx]\n",
    "                    \n",
    "                    # Append data for this body part\n",
    "                    row = [file_number, fname, bp, frame_id, x, y, c, fps, pixel_x, pixel_y, time, part_idx]\n",
    "                    data.append(row)\n",
    "                    interim.append(row)\n",
    "\n",
    "    interim_df = pd.DataFrame(interim, columns=columns)\n",
    "    interim_df.to_pickle(f'{save_path}/{fname}.pkl')\n",
    "\n",
    "# Create DataFrame from the accumulated data\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.to_csv(f'{save_path}/pose_estimates_{dataset}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/pose_estimates_PANDA2B.csv')\n",
    "df_all = pd.read_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/pose_estimates_PANDA2B.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all.video_number.unique()[1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for video in tqdm(df_all.video_number.unique(), desc=f'Processing videos'):\n",
    "\n",
    "    df = df_all[df_all.video_number==video]\n",
    "\n",
    "    median_window = 1\n",
    "    mean_window = 1\n",
    "    delta_window = .25 # smoothing applied to delta_x, velocity, acceleration\n",
    "\n",
    "    # normalise x and y by image length (conserve aspect ratio)\n",
    "    df['x'] = pd.to_numeric(df['x'])\n",
    "    df['y'] = pd.to_numeric(df['y'])\n",
    "    df['x'] = (df['x'] - df['pixel_x']/2)/df['pixel_y']\n",
    "    df['y'] = (df['y'] - df['pixel_y']/2)/df['pixel_y']\n",
    "    # interpolate\n",
    "    df = df.groupby(['video', 'bp']).apply(interpolate_df).reset_index(drop=True)\n",
    "    # median and mean filter\n",
    "    median_window = .5\n",
    "    mean_window = .5\n",
    "    df = df.groupby(['video', 'bp']).apply(lambda x: smooth(x, 'y', median_window, mean_window)).reset_index(drop=True)\n",
    "    df = df.groupby(['video', 'bp']).apply(lambda x: smooth(x, 'x', median_window, mean_window)).reset_index(drop=True)\n",
    "    # rotate and normalise by reference\n",
    "    xdf = normalise_skeletons(df)\n",
    "    # extract angles\n",
    "    adf = get_joint_angles(df)\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # get dynamics\n",
    "    #print(f'Getting XY features...')\n",
    "\n",
    "    xdf = get_dynamics_xy(xdf, delta_window)\n",
    "    #xdf.to_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/processed_pose_estimates_coords.pkl')\n",
    "    xdf.to_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/xdf/{video}_processed_pose_estimates_coords.pkl')\n",
    "   \n",
    "    del xdf\n",
    "    gc.collect()\n",
    "\n",
    "    #print('Getting angle features...')\n",
    "\n",
    "    adf = get_dynamics_angle(adf,delta_window)\n",
    "    #adf.to_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/processed_pose_estimates_angles.pkl')\n",
    "    adf.to_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/adf/{video}_processed_pose_estimates_angles.pkl')\n",
    "\n",
    "    del adf\n",
    "    gc.collect()\n",
    "\n",
    "del df_all\n",
    "gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf_files = '/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/xdf'\n",
    "\n",
    "processed_pose_estimates_coords = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in tqdm(os.listdir(xdf_files)):\n",
    "    if filename.endswith('.pkl'):  # Ensure processing only .pkl files\n",
    "        file_path = os.path.join(xdf_files, filename)\n",
    "        # Read the current DataFrame\n",
    "        temp_df = pd.read_pickle(file_path)\n",
    "        # Concatenate it with the output_df DataFrame\n",
    "        processed_pose_estimates_coords = pd.concat([processed_pose_estimates_coords, temp_df], ignore_index=True)\n",
    "        \n",
    "        del temp_df\n",
    "        gc.collect()\n",
    "\n",
    "processed_pose_estimates_coords.to_pickle('/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/processed_pose_estimates_coords.pkl')\n",
    "\n",
    "del processed_pose_estimates_coords\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_files = '/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/adf'\n",
    "\n",
    "processed_pose_estimates_angles = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in tqdm(os.listdir(adf_files)):\n",
    "    if filename.endswith('.pkl'):  # Ensure processing only .pkl files\n",
    "        file_path = os.path.join(adf_files, filename)\n",
    "        # Read the current DataFrame\n",
    "        temp_df = pd.read_pickle(file_path)\n",
    "        # Concatenate it with the output_df DataFrame\n",
    "        processed_pose_estimates_angles = pd.concat([processed_pose_estimates_angles, temp_df], ignore_index=True)\n",
    "        \n",
    "        del temp_df\n",
    "        gc.collect()\n",
    "\n",
    "processed_pose_estimates_angles.to_pickle('/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/processed_pose_estimates_angles.pkl')\n",
    "\n",
    "del processed_pose_estimates_angles\n",
    "print('saving output!')\n",
    "gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xdf\n",
    "del adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xdf_all = pd.DataFrame()\n",
    "#adf_all = pd.DataFrame()\n",
    "\n",
    "for video in df_all.video_number.unique()[1001:len(df_all.video_number.unique())]:\n",
    "    print(f'Processing video: {video}')\n",
    "\n",
    "    df = df_all[df_all.video_number==video]\n",
    "\n",
    "    median_window = 1\n",
    "    mean_window = 1\n",
    "    delta_window = .25 # smoothing applied to delta_x, velocity, acceleration\n",
    "\n",
    "    # normalise x and y by image length (conserve aspect ratio)\n",
    "    df['x'] = pd.to_numeric(df['x'])\n",
    "    df['y'] = pd.to_numeric(df['y'])\n",
    "    df['x'] = (df['x'] - df['pixel_x']/2)/df['pixel_y']\n",
    "    df['y'] = (df['y'] - df['pixel_y']/2)/df['pixel_y']\n",
    "    # interpolate\n",
    "    df = df.groupby(['video', 'bp']).apply(interpolate_df).reset_index(drop=True)\n",
    "    # median and mean filter\n",
    "    median_window = .5\n",
    "    mean_window = .5\n",
    "    df = df.groupby(['video', 'bp']).apply(lambda x: smooth(x, 'y', median_window, mean_window)).reset_index(drop=True)\n",
    "    df = df.groupby(['video', 'bp']).apply(lambda x: smooth(x, 'x', median_window, mean_window)).reset_index(drop=True)\n",
    "    # rotate and normalise by reference\n",
    "    xdf = normalise_skeletons(df)\n",
    "    # extract angles\n",
    "    adf = get_joint_angles(df)\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # get dynamics\n",
    "    print(f'Getting XY features...')\n",
    "\n",
    "    xdf = get_dynamics_xy(xdf, delta_window)\n",
    "    #xdf.to_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/processed_pose_estimates_coords.pkl')\n",
    "    #xdf_all = pd.concat([xdf_all, xdf])\n",
    "\n",
    "    \n",
    "    print('Getting angle features...')\n",
    "\n",
    "    adf = get_dynamics_angle(adf,delta_window)\n",
    "    #adf.to_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/processed_pose_estimates_angles.pkl')\n",
    "    #adf_all = pd.concat([adf_all, adf])\n",
    "    \n",
    "    xdf.to_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/xdf/{video}_processed_pose_estimates_coords.pkl')\n",
    "    adf.to_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/adf/{video}_processed_pose_estimates_angles.pkl')\n",
    "\n",
    "    del adf\n",
    "    del xdf \n",
    "    gc.collect()\n",
    "    # save\n",
    "    #xdf.to_pickle(os.path.join(pose_estimates_path, 'processed_pose_estimates_coords.pkl'))\n",
    "    #adf.to_pickle(os.path.join(pose_estimates_path, 'processed_pose_estimates_angles.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf = pd.read_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/processed_pose_estimates_angles.pkl')\n",
    "\n",
    "# angle features\n",
    "feature_angle = adf.groupby(['bp','video']).apply(angle_features).reset_index(drop=True)\n",
    "feature_angle = pd.pivot_table(feature_angle, index='video', columns=['bp'])\n",
    "l0 = feature_angle.columns.get_level_values(1)\n",
    "l1 = feature_angle.columns.get_level_values(0)\n",
    "cols = [l1[i]+'_'+l0[i] for i in range(len(l1))]\n",
    "feature_angle.columns = cols\n",
    "feature_angle =feature_angle.reset_index()\n",
    "# - measure of symmetry (left-right cross correlation)\n",
    "corr_joint = adf.groupby(['video', 'part']).apply(lambda x:corr_lr(x,'angle')).reset_index()\n",
    "corr_joint['part'] = 'lrCorr_angle_'+corr_joint['part']\n",
    "corr_joint.columns = ['video', 'feature', 'Value']\n",
    "corr_joint = pd.pivot_table(corr_joint, index='video', columns=['feature'])\n",
    "l1 = corr_joint.columns.get_level_values(1)\n",
    "corr_joint.columns = l1\n",
    "corr_joint = corr_joint.reset_index()\n",
    "feature_angle = pd.merge(feature_angle,corr_joint, on='video', how='outer')\n",
    "\n",
    "del adf \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf = pd.read_pickle(f'/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/processed_pose_estimates_coords.pkl')\n",
    "\n",
    "# xy features\n",
    "bps = ['LAnkle', 'RAnkle', 'LWrist', 'RWrist']\n",
    "feature_xy = xdf[np.isin(xdf.bp, bps)].groupby(['bp','video']).apply(xy_features).reset_index(drop=True)\n",
    "feature_xy = pd.pivot_table(feature_xy, index='video', columns=['bp'])\n",
    "l0 = feature_xy.columns.get_level_values(1)\n",
    "l1 = feature_xy.columns.get_level_values(0)\n",
    "cols = [l1[i]+'_'+l0[i] for i in range(len(l1))]\n",
    "feature_xy.columns = cols\n",
    "feature_xy = feature_xy.reset_index()\n",
    "# - measure of symmetry (left-right cross correlation)\n",
    "xdf['dist'] = np.sqrt(xdf['x']**2+xdf['y']**2)\n",
    "corr_joint = xdf.groupby(['video', 'part']).apply(lambda x:corr_lr(x,'dist')).reset_index()\n",
    "corr_joint['part'] = 'lrCorr_x_'+corr_joint['part']\n",
    "corr_joint.columns = ['video', 'feature', 'Value']\n",
    "corr_joint = pd.pivot_table(corr_joint, index='video', columns=['feature'])\n",
    "l1 = corr_joint.columns.get_level_values(1)\n",
    "corr_joint.columns = l1\n",
    "corr_joint = corr_joint.reset_index()\n",
    "feature_xy = pd.merge(feature_xy, corr_joint, on='video', how='outer')\n",
    "features = pd.merge(feature_xy, feature_angle, on='video', how='outer')\n",
    "\n",
    "del xdf \n",
    "gc.collect()\n",
    "\n",
    "features.to_pickle('/workspaces/wiggleformer/data/PANDA/pose_estimates/PANDA2B/features_PANDA2B.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gma_id</th>\n",
       "      <th>zip</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_gest_weeks</th>\n",
       "      <th>age_gest_days</th>\n",
       "      <th>birth_weight_grams</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>payor</th>\n",
       "      <th>chop_sdu</th>\n",
       "      <th>...</th>\n",
       "      <th>rvwr1_video_quality</th>\n",
       "      <th>rvwr1_assessment_2</th>\n",
       "      <th>rvwr2_video_quality</th>\n",
       "      <th>rvwr2_assessment_2</th>\n",
       "      <th>adj_video_quality</th>\n",
       "      <th>final_assessment_score_2</th>\n",
       "      <th>cdw_race</th>\n",
       "      <th>cdw_ethnicity</th>\n",
       "      <th>cp_smarttext</th>\n",
       "      <th>hrcp_smarttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>1000</td>\n",
       "      <td>19020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1000</td>\n",
       "      <td>19020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>1000</td>\n",
       "      <td>19020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gma_id    zip  sex  age_gest_weeks  age_gest_days  birth_weight_grams  \\\n",
       "878    1000  19020  1.0            28.0            3.0                 NaN   \n",
       "879    1000  19020  1.0            28.0            3.0                 NaN   \n",
       "880    1000  19020  1.0            28.0            3.0                 NaN   \n",
       "\n",
       "     ethnicity  race  payor  chop_sdu  ...  rvwr1_video_quality  \\\n",
       "878        2.0   2.0    2.0       0.0  ...                  1.0   \n",
       "879        2.0   2.0    2.0       0.0  ...                  1.0   \n",
       "880        2.0   2.0    2.0       0.0  ...                  1.0   \n",
       "\n",
       "     rvwr1_assessment_2  rvwr2_video_quality  rvwr2_assessment_2  \\\n",
       "878                 1.0                  1.0                 1.0   \n",
       "879                 NaN                  1.0                 NaN   \n",
       "880                 NaN                  NaN                 NaN   \n",
       "\n",
       "     adj_video_quality  final_assessment_score_2 cdw_race  \\\n",
       "878                1.0                       1.0    Asian   \n",
       "879                1.0                       NaN    Asian   \n",
       "880                1.0                       NaN    Asian   \n",
       "\n",
       "                  cdw_ethnicity  cp_smarttext  hrcp_smarttext  \n",
       "878  Non-hispanic Or Non-latino           NaN             NaN  \n",
       "879  Non-hispanic Or Non-latino           NaN             NaN  \n",
       "880  Non-hispanic Or Non-latino           NaN             NaN  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data_PANDA2B = pd.read_csv('/workspaces/wiggleformer/data/PANDA/gma_no_phi_2024.csv')\n",
    "\n",
    "meta_data_PANDA2B[meta_data_PANDA2B['gma_id'] == 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>IQRaccx_LAnkle</th>\n",
       "      <th>IQRaccx_LWrist</th>\n",
       "      <th>IQRaccx_RAnkle</th>\n",
       "      <th>IQRaccx_RWrist</th>\n",
       "      <th>IQRaccy_LAnkle</th>\n",
       "      <th>IQRaccy_LWrist</th>\n",
       "      <th>IQRaccy_RAnkle</th>\n",
       "      <th>IQRaccy_RWrist</th>\n",
       "      <th>IQRvelx_LAnkle</th>\n",
       "      <th>...</th>\n",
       "      <th>stdev_angle_RHip</th>\n",
       "      <th>stdev_angle_RKnee</th>\n",
       "      <th>stdev_angle_RShoulder</th>\n",
       "      <th>lrCorr_angle_Elbow</th>\n",
       "      <th>lrCorr_angle_Hip</th>\n",
       "      <th>lrCorr_angle_Knee</th>\n",
       "      <th>lrCorr_angle_Shoulder</th>\n",
       "      <th>infant</th>\n",
       "      <th>session</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000_1__age_36_42_week_video</td>\n",
       "      <td>0.299349</td>\n",
       "      <td>0.517306</td>\n",
       "      <td>0.347857</td>\n",
       "      <td>0.584199</td>\n",
       "      <td>0.392278</td>\n",
       "      <td>0.638857</td>\n",
       "      <td>0.480387</td>\n",
       "      <td>0.682865</td>\n",
       "      <td>0.098987</td>\n",
       "      <td>...</td>\n",
       "      <td>2.327128</td>\n",
       "      <td>4.017956</td>\n",
       "      <td>2.607487</td>\n",
       "      <td>0.260668</td>\n",
       "      <td>0.440274</td>\n",
       "      <td>-0.091075</td>\n",
       "      <td>0.558510</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000_2__age_36_42_week_video</td>\n",
       "      <td>0.221624</td>\n",
       "      <td>0.201708</td>\n",
       "      <td>0.215364</td>\n",
       "      <td>0.174813</td>\n",
       "      <td>0.243407</td>\n",
       "      <td>0.218713</td>\n",
       "      <td>0.220667</td>\n",
       "      <td>0.183955</td>\n",
       "      <td>0.053802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559773</td>\n",
       "      <td>1.006905</td>\n",
       "      <td>1.110936</td>\n",
       "      <td>0.155990</td>\n",
       "      <td>-0.192380</td>\n",
       "      <td>0.097161</td>\n",
       "      <td>0.057485</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000_3__age_3_4_month_video</td>\n",
       "      <td>0.313598</td>\n",
       "      <td>0.194696</td>\n",
       "      <td>0.308874</td>\n",
       "      <td>0.200277</td>\n",
       "      <td>0.360432</td>\n",
       "      <td>0.198129</td>\n",
       "      <td>0.413967</td>\n",
       "      <td>0.136788</td>\n",
       "      <td>0.077593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788850</td>\n",
       "      <td>1.361817</td>\n",
       "      <td>0.818355</td>\n",
       "      <td>0.138516</td>\n",
       "      <td>0.186654</td>\n",
       "      <td>-0.101794</td>\n",
       "      <td>0.360744</td>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001_1__age_36_42_week_video</td>\n",
       "      <td>0.133566</td>\n",
       "      <td>0.140665</td>\n",
       "      <td>0.143311</td>\n",
       "      <td>0.173298</td>\n",
       "      <td>0.150724</td>\n",
       "      <td>0.171008</td>\n",
       "      <td>0.149625</td>\n",
       "      <td>0.129998</td>\n",
       "      <td>0.035924</td>\n",
       "      <td>...</td>\n",
       "      <td>2.249397</td>\n",
       "      <td>1.494462</td>\n",
       "      <td>1.267880</td>\n",
       "      <td>-0.142535</td>\n",
       "      <td>0.594425</td>\n",
       "      <td>0.006842</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001_2__age_3_4_month_video</td>\n",
       "      <td>0.595316</td>\n",
       "      <td>0.395201</td>\n",
       "      <td>0.543096</td>\n",
       "      <td>0.366514</td>\n",
       "      <td>0.787088</td>\n",
       "      <td>0.344332</td>\n",
       "      <td>0.642379</td>\n",
       "      <td>0.358831</td>\n",
       "      <td>0.135384</td>\n",
       "      <td>...</td>\n",
       "      <td>1.178411</td>\n",
       "      <td>1.978798</td>\n",
       "      <td>0.703052</td>\n",
       "      <td>0.458593</td>\n",
       "      <td>0.646392</td>\n",
       "      <td>0.556526</td>\n",
       "      <td>0.398665</td>\n",
       "      <td>1001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>99_1__age_3_4_month_video</td>\n",
       "      <td>0.433914</td>\n",
       "      <td>0.645454</td>\n",
       "      <td>0.417005</td>\n",
       "      <td>1.088822</td>\n",
       "      <td>0.481144</td>\n",
       "      <td>0.904901</td>\n",
       "      <td>0.438909</td>\n",
       "      <td>1.020525</td>\n",
       "      <td>0.110441</td>\n",
       "      <td>...</td>\n",
       "      <td>2.713450</td>\n",
       "      <td>1.722524</td>\n",
       "      <td>2.356358</td>\n",
       "      <td>0.063082</td>\n",
       "      <td>0.932148</td>\n",
       "      <td>0.839802</td>\n",
       "      <td>0.688296</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>9_1__age_36_42_week_video</td>\n",
       "      <td>0.151131</td>\n",
       "      <td>0.411242</td>\n",
       "      <td>0.165891</td>\n",
       "      <td>0.253599</td>\n",
       "      <td>0.194879</td>\n",
       "      <td>0.359731</td>\n",
       "      <td>0.241878</td>\n",
       "      <td>0.294249</td>\n",
       "      <td>0.037303</td>\n",
       "      <td>...</td>\n",
       "      <td>1.086033</td>\n",
       "      <td>1.582757</td>\n",
       "      <td>1.093521</td>\n",
       "      <td>0.193890</td>\n",
       "      <td>-0.222496</td>\n",
       "      <td>-0.464923</td>\n",
       "      <td>0.491738</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>9_2__age_3_4_month_video</td>\n",
       "      <td>0.234048</td>\n",
       "      <td>0.271844</td>\n",
       "      <td>0.304607</td>\n",
       "      <td>0.339485</td>\n",
       "      <td>0.256133</td>\n",
       "      <td>0.346557</td>\n",
       "      <td>0.317208</td>\n",
       "      <td>0.389928</td>\n",
       "      <td>0.054312</td>\n",
       "      <td>...</td>\n",
       "      <td>1.090768</td>\n",
       "      <td>1.447111</td>\n",
       "      <td>0.940503</td>\n",
       "      <td>0.304572</td>\n",
       "      <td>0.671315</td>\n",
       "      <td>0.173842</td>\n",
       "      <td>0.504079</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>9_3__age_3_4_month_video</td>\n",
       "      <td>0.440481</td>\n",
       "      <td>0.544459</td>\n",
       "      <td>0.390104</td>\n",
       "      <td>0.338592</td>\n",
       "      <td>0.462288</td>\n",
       "      <td>0.627356</td>\n",
       "      <td>1.048151</td>\n",
       "      <td>0.429019</td>\n",
       "      <td>0.130246</td>\n",
       "      <td>...</td>\n",
       "      <td>2.125455</td>\n",
       "      <td>2.306015</td>\n",
       "      <td>0.687916</td>\n",
       "      <td>-0.190200</td>\n",
       "      <td>0.373279</td>\n",
       "      <td>0.079295</td>\n",
       "      <td>0.184817</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>9_4__age_3_4_month_video</td>\n",
       "      <td>0.572953</td>\n",
       "      <td>0.989586</td>\n",
       "      <td>0.713284</td>\n",
       "      <td>0.547840</td>\n",
       "      <td>0.928965</td>\n",
       "      <td>0.755587</td>\n",
       "      <td>1.456575</td>\n",
       "      <td>0.563653</td>\n",
       "      <td>0.167448</td>\n",
       "      <td>...</td>\n",
       "      <td>1.726342</td>\n",
       "      <td>2.081540</td>\n",
       "      <td>0.621385</td>\n",
       "      <td>0.064108</td>\n",
       "      <td>0.647275</td>\n",
       "      <td>0.447057</td>\n",
       "      <td>0.287371</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2977 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             video  IQRaccx_LAnkle  IQRaccx_LWrist  \\\n",
       "0     1000_1__age_36_42_week_video        0.299349        0.517306   \n",
       "1     1000_2__age_36_42_week_video        0.221624        0.201708   \n",
       "2      1000_3__age_3_4_month_video        0.313598        0.194696   \n",
       "3     1001_1__age_36_42_week_video        0.133566        0.140665   \n",
       "4      1001_2__age_3_4_month_video        0.595316        0.395201   \n",
       "...                            ...             ...             ...   \n",
       "2972     99_1__age_3_4_month_video        0.433914        0.645454   \n",
       "2973     9_1__age_36_42_week_video        0.151131        0.411242   \n",
       "2974      9_2__age_3_4_month_video        0.234048        0.271844   \n",
       "2975      9_3__age_3_4_month_video        0.440481        0.544459   \n",
       "2976      9_4__age_3_4_month_video        0.572953        0.989586   \n",
       "\n",
       "      IQRaccx_RAnkle  IQRaccx_RWrist  IQRaccy_LAnkle  IQRaccy_LWrist  \\\n",
       "0           0.347857        0.584199        0.392278        0.638857   \n",
       "1           0.215364        0.174813        0.243407        0.218713   \n",
       "2           0.308874        0.200277        0.360432        0.198129   \n",
       "3           0.143311        0.173298        0.150724        0.171008   \n",
       "4           0.543096        0.366514        0.787088        0.344332   \n",
       "...              ...             ...             ...             ...   \n",
       "2972        0.417005        1.088822        0.481144        0.904901   \n",
       "2973        0.165891        0.253599        0.194879        0.359731   \n",
       "2974        0.304607        0.339485        0.256133        0.346557   \n",
       "2975        0.390104        0.338592        0.462288        0.627356   \n",
       "2976        0.713284        0.547840        0.928965        0.755587   \n",
       "\n",
       "      IQRaccy_RAnkle  IQRaccy_RWrist  IQRvelx_LAnkle  ...  stdev_angle_RHip  \\\n",
       "0           0.480387        0.682865        0.098987  ...          2.327128   \n",
       "1           0.220667        0.183955        0.053802  ...          0.559773   \n",
       "2           0.413967        0.136788        0.077593  ...          0.788850   \n",
       "3           0.149625        0.129998        0.035924  ...          2.249397   \n",
       "4           0.642379        0.358831        0.135384  ...          1.178411   \n",
       "...              ...             ...             ...  ...               ...   \n",
       "2972        0.438909        1.020525        0.110441  ...          2.713450   \n",
       "2973        0.241878        0.294249        0.037303  ...          1.086033   \n",
       "2974        0.317208        0.389928        0.054312  ...          1.090768   \n",
       "2975        1.048151        0.429019        0.130246  ...          2.125455   \n",
       "2976        1.456575        0.563653        0.167448  ...          1.726342   \n",
       "\n",
       "      stdev_angle_RKnee  stdev_angle_RShoulder  lrCorr_angle_Elbow  \\\n",
       "0              4.017956               2.607487            0.260668   \n",
       "1              1.006905               1.110936            0.155990   \n",
       "2              1.361817               0.818355            0.138516   \n",
       "3              1.494462               1.267880           -0.142535   \n",
       "4              1.978798               0.703052            0.458593   \n",
       "...                 ...                    ...                 ...   \n",
       "2972           1.722524               2.356358            0.063082   \n",
       "2973           1.582757               1.093521            0.193890   \n",
       "2974           1.447111               0.940503            0.304572   \n",
       "2975           2.306015               0.687916           -0.190200   \n",
       "2976           2.081540               0.621385            0.064108   \n",
       "\n",
       "      lrCorr_angle_Hip  lrCorr_angle_Knee  lrCorr_angle_Shoulder  infant  \\\n",
       "0             0.440274          -0.091075               0.558510    1000   \n",
       "1            -0.192380           0.097161               0.057485    1000   \n",
       "2             0.186654          -0.101794               0.360744    1000   \n",
       "3             0.594425           0.006842               0.003784    1001   \n",
       "4             0.646392           0.556526               0.398665    1001   \n",
       "...                ...                ...                    ...     ...   \n",
       "2972          0.932148           0.839802               0.688296      99   \n",
       "2973         -0.222496          -0.464923               0.491738       9   \n",
       "2974          0.671315           0.173842               0.504079       9   \n",
       "2975          0.373279           0.079295               0.184817       9   \n",
       "2976          0.647275           0.447057               0.287371       9   \n",
       "\n",
       "      session  age_group  \n",
       "0           1          0  \n",
       "1           2          0  \n",
       "2           3          1  \n",
       "3           1          0  \n",
       "4           2          1  \n",
       "...       ...        ...  \n",
       "2972        1          1  \n",
       "2973        1          0  \n",
       "2974        2          1  \n",
       "2975        3          1  \n",
       "2976        4          1  \n",
       "\n",
       "[2977 rows x 108 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['infant'] = features['video'].str.split('_').str[0].astype('Int64')\n",
    "features['session'] = features['video'].str.split('_').str[1].astype('Int64')\n",
    "features['age_group'] = features['video'].apply(lambda x: '0' if x.split('_')[4] == '36' else '1')\n",
    "\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building meta data for PANDA2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'gma_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/miniconda/envs/openmmlab/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda/envs/openmmlab/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gma_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuilding meta data for PANDA2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m meta_info_panda2 \u001b[38;5;241m=\u001b[39m meta_data_PANDA2B\n\u001b[0;32m----> 4\u001b[0m meta_info_panda2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_info_panda2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgma_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m meta_info_panda2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m meta_info_panda2\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfant\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcumcount()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate months and remaining weeks, handling NaN values\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/miniconda/envs/openmmlab/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gma_id'"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Building meta data for PANDA2')\n",
    "\n",
    "meta_info_panda2 = meta_data_PANDA2B\n",
    "meta_info_panda2['infant'] = meta_info_panda2['gma_id']\n",
    "meta_info_panda2['session'] = meta_info_panda2.groupby('infant').cumcount().astype(str)\n",
    "# Calculate months and remaining weeks, handling NaN values\n",
    "months_chron = meta_info_panda2['chrono_age_on_upload_date'] / 4.345\n",
    "months_corr = meta_info_panda2['corr_age_on_upload_date'] / 4.345\n",
    "\n",
    "days_chron_remaining = np.mod(meta_info_panda2['chrono_age_on_upload_date'], 4.345)*7\n",
    "days_corr_remaining = np.mod(meta_info_panda2['corr_age_on_upload_date'], 4.345)*7\n",
    "\n",
    "meta_info_panda2['Months_chron'] = np.where(np.isnan(months_chron), np.nan, np.floor(months_chron))\n",
    "meta_info_panda2['Days_chron'] = days_chron_remaining\n",
    "meta_info_panda2['Days_chron'] = np.where(np.isnan(days_chron_remaining), np.nan, np.floor(days_chron_remaining))\n",
    "\n",
    "# Calculate months and remaining weeks, handling NaN values\n",
    "months_chron = meta_info_panda2['chrono_age_on_upload_date'] / 4.345\n",
    "days_chron_remaining = np.mod(meta_info_panda2['chrono_age_on_upload_date'], 4.345)*7\n",
    "\n",
    "meta_info_panda2['Months_corr'] = np.where(np.isnan(months_corr), np.nan, np.floor(months_corr))\n",
    "meta_info_panda2['Days_corr'] = days_corr_remaining\n",
    "meta_info_panda2['Days_chron'] = np.where(np.isnan(days_corr_remaining), np.nan, np.floor(days_corr_remaining))\n",
    "\n",
    "meta_info_panda2.rename(columns={'gma_id':'video', 'final_assessment_score_2':'Risk_low0_mod1_high2_chron' }, inplace=True)\n",
    "meta_info_panda2['Risk_low0_mod1_high2_corr'] = meta_info_panda2['Risk_low0_mod1_high2_chron']\n",
    "\n",
    "meta_info_panda2 = meta_info_panda2[['infant', 'session', 'Months_chron', 'Days_chron', 'Risk_low0_mod1_high2_chron', 'Months_corr', 'Days_corr', 'Risk_low0_mod1_high2_corr','video']]\n",
    "\n",
    "\n",
    "\n",
    "#meta_info_panda2.to_pickle(f'{meta_data_path}/meta_data_PANDA2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>zip</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_gest_weeks</th>\n",
       "      <th>age_gest_days</th>\n",
       "      <th>birth_weight_grams</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>payor</th>\n",
       "      <th>chop_sdu</th>\n",
       "      <th>...</th>\n",
       "      <th>cdw_ethnicity</th>\n",
       "      <th>cp_smarttext</th>\n",
       "      <th>hrcp_smarttext</th>\n",
       "      <th>infant</th>\n",
       "      <th>session</th>\n",
       "      <th>Months_chron</th>\n",
       "      <th>Days_chron</th>\n",
       "      <th>Months_corr</th>\n",
       "      <th>Days_corr</th>\n",
       "      <th>Risk_low0_mod1_high2_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8096</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.745</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>19023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3385.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>19023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3385.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.360</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>19020</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3380.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.030</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>19020</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3380.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.755</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>99311</td>\n",
       "      <td>19153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>992.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99311</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20.405</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>99311</td>\n",
       "      <td>19153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>992.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99311</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10.395</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>99405</td>\n",
       "      <td>8536</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2725.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99405</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.970</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>99598</td>\n",
       "      <td>17922</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2185.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Hispanic Or Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99598</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>99722</td>\n",
       "      <td>19131</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2430.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-hispanic Or Non-latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99722</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.775</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2998 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video    zip  sex  age_gest_weeks  age_gest_days  birth_weight_grams  \\\n",
       "0         0   8096  2.0            23.0            3.0               610.0   \n",
       "1         2  19023  2.0            39.0            6.0              3385.0   \n",
       "2         2  19023  2.0            39.0            6.0              3385.0   \n",
       "3         3  19020  2.0            38.0            3.0              3380.0   \n",
       "4         3  19020  2.0            38.0            3.0              3380.0   \n",
       "...     ...    ...  ...             ...            ...                 ...   \n",
       "2993  99311  19153  1.0            32.0            4.0               992.0   \n",
       "2994  99311  19153  1.0            32.0            4.0               992.0   \n",
       "2995  99405   8536  1.0            37.0            0.0              2725.0   \n",
       "2996  99598  17922  1.0            37.0            2.0              2185.0   \n",
       "2997  99722  19131  2.0            37.0            6.0              2430.0   \n",
       "\n",
       "      ethnicity  race  payor  chop_sdu  ...               cdw_ethnicity  \\\n",
       "0          99.0   9.0    1.0       0.0  ...  Non-hispanic Or Non-latino   \n",
       "1           2.0   3.0   99.0       0.0  ...  Non-hispanic Or Non-latino   \n",
       "2           2.0   3.0   99.0       0.0  ...  Non-hispanic Or Non-latino   \n",
       "3          99.0   9.0    2.0       0.0  ...  Non-hispanic Or Non-latino   \n",
       "4          99.0   9.0    2.0       0.0  ...  Non-hispanic Or Non-latino   \n",
       "...         ...   ...    ...       ...  ...                         ...   \n",
       "2993       99.0   3.0    2.0       0.0  ...  Non-hispanic Or Non-latino   \n",
       "2994       99.0   3.0    2.0       0.0  ...  Non-hispanic Or Non-latino   \n",
       "2995       99.0   2.0    2.0       0.0  ...  Non-hispanic Or Non-latino   \n",
       "2996       99.0   9.0    1.0       1.0  ...          Hispanic Or Latino   \n",
       "2997       99.0   3.0    1.0       0.0  ...  Non-hispanic Or Non-latino   \n",
       "\n",
       "      cp_smarttext  hrcp_smarttext  infant  session  Months_chron Days_chron  \\\n",
       "0              NaN             NaN       0        0           7.0       10.0   \n",
       "1              NaN             NaN       2        0           0.0       14.0   \n",
       "2              NaN             NaN       2        1           4.0       24.0   \n",
       "3              NaN             NaN       3        0           0.0       16.0   \n",
       "4              NaN             NaN       3        1           3.0        6.0   \n",
       "...            ...             ...     ...      ...           ...        ...   \n",
       "2993           NaN             NaN   99311        0           1.0       20.0   \n",
       "2994           NaN             NaN   99311        1           1.0       10.0   \n",
       "2995           NaN             NaN   99405        0           0.0       25.0   \n",
       "2996           NaN             NaN   99598        0           1.0        4.0   \n",
       "2997           NaN             NaN   99722        0           3.0       19.0   \n",
       "\n",
       "      Months_corr  Days_corr  Risk_low0_mod1_high2_corr  \n",
       "0             3.0     10.745                        1.0  \n",
       "1             0.0     14.000                        NaN  \n",
       "2             4.0     24.360                        1.0  \n",
       "3             0.0     16.030                        NaN  \n",
       "4             3.0      6.755                        1.0  \n",
       "...           ...        ...                        ...  \n",
       "2993         -1.0     20.405                        NaN  \n",
       "2994         -1.0     10.395                        NaN  \n",
       "2995          0.0     25.970                        NaN  \n",
       "2996          1.0      4.585                        NaN  \n",
       "2997          3.0     19.775                        1.0  \n",
       "\n",
       "[2998 rows x 50 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_all = pd.merge(features, meta_info_panda2, on=['infant','session'], how='inner')\n",
    "meta_info_panda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Load the new coco-formatted pose estimates (1 json file per video)\n",
    "\n",
    "dataset = 'CLIN'\n",
    "json_path = f'../outputs/{dataset}/openpose/annotations'\n",
    "json_files = os.listdir(json_path)\n",
    "directory = f'../outputs/{dataset}/openpose'\n",
    "\n",
    "save_path = f'../pose_estimates/{dataset}'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "kp_mapping = {0:'Nose', 1:'Neck', 2:'RShoulder', 3:'RElbow', 4:'RWrist', 5:'LShoulder', 6:'LElbow', \n",
    "              7:'LWrist', 8:'RHip', 9:'RKnee', 10:'RAnkle', 11:'LHip', \n",
    "              12:'LKnee', 13:'LAnkle', 14:'REye', 15:'LEye', 16:'REar', 17:'LEar'}\n",
    "\n",
    "# Define the DataFrame columns as specified\n",
    "columns = ['video_number', 'video', 'bp', 'frame', 'x', 'y', 'c','fps', 'pixel_x', 'pixel_y', 'time', 'part_idx']\n",
    "#vid_info = pd.read_csv('../data/vid_info.csv')\n",
    "\n",
    "for file_number, file in enumerate(json_files, start=0):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(json_path, file)\n",
    "    fname = file.split('.')[0]\n",
    "    \n",
    "\n",
    "    vis_path = find_file_by_basename(f'{directory}/vis', fname)\n",
    "\n",
    "    # Get video info\n",
    "    try:\n",
    "        width, height, center_x, center_y, fps = get_orig_video_info(vis_path)\n",
    "        fps = fps\n",
    "        pixel_x = width\n",
    "        pixel_y = height\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    if fps == 0:\n",
    "        continue\n",
    "\n",
    "    # Open and load the JSON data\n",
    "    with open(file_path, 'r') as f:\n",
    "        frames = json.load(f)\n",
    "        \n",
    "        # Iterate through each frame in the JSON file\n",
    "        for frame in frames:\n",
    "            frame_id = frame['frame_id']\n",
    "            if 'instances' in frame and len(frame['instances']) > 0:\n",
    "                keypoints = frame['instances'][0]['keypoints']\n",
    "                confidence = frame['instances'][0]['keypoint_scores']\n",
    "                keypoints, confidence = convert_coco_to_openpose(keypoints, confidence)\n",
    "\n",
    "                # Iterate through each keypoint\n",
    "                for part_idx, (x, y) in enumerate(keypoints):\n",
    "                    # Assuming you have a way to map part_idx to a body part name 'bp'\n",
    "\n",
    "                    bp = kp_mapping[part_idx]  \n",
    "                    fps = fps  \n",
    "                    time = frame_id / fps\n",
    "                    c = confidence[part_idx]\n",
    "                    \n",
    "                    # Append data for this body part\n",
    "                    row = [file_number, fname, bp, frame_id, x, y, c, fps, pixel_x, pixel_y, time, part_idx]\n",
    "                    data.append(row)\n",
    "\n",
    "        # Create DataFrame from the accumulated data\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df.to_csv(f'{save_path}/pose_estimates_{dataset}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limbSeq = [\n",
    "            [2, 3], [2, 6], [3, 4], [4, 5], [6, 7], [7, 8], [2, 9], [9, 10],\n",
    "            [10, 11], [2, 12], [12, 13], [13, 14], [2, 1], [1, 15], [15, 17],\n",
    "            [1, 16], [16, 18], [3, 17], [6, 18]\n",
    "        ]\n",
    "\n",
    "colors = [\n",
    "            [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], \n",
    "            [170, 255, 0], [85, 255, 0], [0, 255, 0], [0, 255, 85], \n",
    "            [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], \n",
    "            [0, 0, 255], [85, 0, 255], [170, 0, 255], [255, 0, 255], \n",
    "            [255, 0, 170], [255, 0, 85], [255, 0, 0]\n",
    "        ]\n",
    "\n",
    "## FIX THIS TOMORROW SO IT JUST DOES ONE VID AT A TIME\n",
    "def plot_skel(df, vid, frame,ax,xvar,yvar,limbSeq,colors):\n",
    "    alpha =.7\n",
    "    df = df[(df.frame==frame)].reset_index(drop=True)\n",
    "    for i,limb in enumerate(limbSeq):\n",
    "        l1 = limb[0]-1; l2 = limb[1]-1;\n",
    "        if (len(df[df.part_idx==l1])>0) & (len(df[df.part_idx==l2])>0):\n",
    "            ax.plot([df[df.part_idx==l1][xvar].iloc[0],df[df.part_idx==l2][xvar].iloc[0]], [df[df.part_idx==l1][yvar].iloc[0],df[df.part_idx==l2][yvar].iloc[0]],linewidth=5, color=[j/255 for j in colors[i]], alpha=alpha)\n",
    "    # plot kp\n",
    "    for i in range(len(df)):\n",
    "        ax.plot(df[xvar][i],df[yvar][i], 'o',markersize = 10, color=[j/255 for j in colors[i]], alpha=alpha)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "plot_skel(df, df['video'].iloc[0],df['frame'].iloc[0],ax,'x','y',limbSeq,colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##look at displacement for each keypoint\n",
    "\n",
    "def calculate_inter_frame_displacements(block):\n",
    "    \"\"\"\n",
    "    Calculate the displacement of each keypoint between consecutive frames in a block.\n",
    "    \n",
    "    :param block: A list of frames, each frame is a dictionary with 'first_detection_keypoints'.\n",
    "    :return: A list of numpy arrays, each array contains the displacements for a keypoint over time.\n",
    "    \"\"\"\n",
    "    keypoints_array = np.array([frame['first_detection_keypoints'] for frame in block])\n",
    "    displacements = np.diff(keypoints_array, axis=0)\n",
    "    inter_frame_displacements = np.linalg.norm(displacements, axis=2)\n",
    "    return inter_frame_displacements.T  # Transpose to have keypoints as rows\n",
    "\n",
    "\n",
    "def plot_keypoint_displacements(displacements, block_id=0, axs=None, color=None):\n",
    "    \"\"\"\n",
    "    Plot the inter-frame displacements for each keypoint on separate y-axes.\n",
    "    \n",
    "    :param displacements: A list of numpy arrays with displacements for each keypoint.\n",
    "    :param block_id: Identifier for the block being plotted (for title).\n",
    "    \"\"\"\n",
    "    num_keypoints = displacements.shape[0]\n",
    "    num_frames = displacements.shape[1]\n",
    "    \n",
    "    # Create a figure and subplots\n",
    "    \n",
    "    \n",
    "    # Plot each keypoint's displacement on its own subplot\n",
    "    for i in range(num_keypoints):\n",
    "        axs[i].plot(range(1, num_frames + 1), displacements[i], label=f'Keypoint {i}', color=color)\n",
    "        axs[i].set_ylabel(f'Kpt {i}')\n",
    "        #axs[i].legend(loc=\"upper right\")\n",
    "    \n",
    "    plt.xlabel('Frame Number')\n",
    "    #fig.suptitle(f'Inter-Frame Displacements for Block {block_id}')\n",
    "\n",
    "num_keypoints = 17\n",
    "fig, axs = plt.subplots(num_keypoints, 1, sharex=True, figsize=(10, 2 * num_keypoints))\n",
    "color_range = plt.cm.viridis(np.linspace(0, 1, len(movement_blocks)))\n",
    "\n",
    "for e, block in enumerate(movement_blocks):\n",
    "    inter_frame_displacements = calculate_inter_frame_displacements(block)\n",
    "    plot_keypoint_displacements(inter_frame_displacements, block_id=e, axs=axs, color=color_range[e])  \n",
    "\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orig_dimensions(file):\n",
    "    file_path = file  # change to your own video path\n",
    "    vid = cv2.VideoCapture(file_path)\n",
    "    height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    width = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "\n",
    "    center_x = (width) / 2\n",
    "    center_y = (height) / 2\n",
    "\n",
    "    return width, height, center_x, center_y\n",
    "\n",
    "def find_first_frame_center(block):\n",
    "\n",
    "    first_frame = block[0]['first_detection_keypoints']\n",
    "    first_frame = reorder_keypoints(first_frame, mapping)\n",
    "    first_frame = np.array(first_frame)\n",
    "\n",
    "    center_x = (first_frame[5][0] + first_frame[6][0]) / 2\n",
    "    center_y = (first_frame[5][1] + first_frame[6][1]) / 2\n",
    "\n",
    "    return center_x, center_y\n",
    "\n",
    "def find_first_frame_rotation(block):\n",
    "    first_frame = block[0]['first_detection_keypoints']\n",
    "    first_frame = reorder_keypoints(first_frame, mapping)\n",
    "    first_frame = np.array(first_frame)\n",
    "\n",
    "    angle_to_vertical = np.arctan((first_frame[5][1] - first_frame[6][1]) / (first_frame[5][0] - first_frame[6][0]))\n",
    "\n",
    "    return angle_to_vertical\n",
    "\n",
    "def get_global_min_max(block):\n",
    "    min_x, min_y = np.inf, np.inf  # Initialize min_x to positive infinity\n",
    "    max_x, max_y = -np.inf, -np.inf  # Initialize max_x to negative infinity\n",
    "\n",
    "    for e, frame in enumerate(block):\n",
    "        frame_keypoints = np.array(frame['first_detection_keypoints'])\n",
    "\n",
    "        min_x_frame, max_x_frame = np.min(frame_keypoints[:, 0]), np.max(frame_keypoints[:, 0])\n",
    "        min_y_frame, max_y_frame = np.min(frame_keypoints[:, 1]), np.max(frame_keypoints[:, 1])\n",
    "\n",
    "        min_x, max_x = min(min_x, min_x_frame), max(max_x, max_x_frame)\n",
    "        min_y, max_y = min(min_y, min_y_frame), max(max_y, max_y_frame)\n",
    "\n",
    "    return min_x, max_x, min_y, max_y\n",
    "\n",
    "def adjust_bounds(block, x_dim, y_dim, displacement_x, displacement_y):\n",
    "   # get the global min and max of the block\n",
    "    min_x, max_x, min_y, max_y = get_global_min_max(block)\n",
    "    \n",
    "    new_min_x, new_max_x = (min_x + displacement_x), (max_x + displacement_x)\n",
    "    new_min_y, new_max_y = (min_y + displacement_y), (max_y + displacement_y)\n",
    "\n",
    "    if new_min_x < 0:\n",
    "        displacement_x = displacement_x + np.abs(min_x)\n",
    "    if new_min_y < 0:\n",
    "        displacement_y = displacement_y + np.abs(min_y)\n",
    "\n",
    "    if new_max_x >= x_dim:\n",
    "        displacement_x = displacement_x - np.abs(new_max_x-x_dim)\n",
    "    if new_max_y >=y_dim:\n",
    "        displacement_y = displacement_y - np.abs(max_y-y_dim)\n",
    "\n",
    "    return displacement_x, displacement_y\n",
    "\n",
    "def get_block_displacement(block, image_center_x, image_center_y, x_dim, y_dim):\n",
    "    \n",
    "    block = np.array(block) \n",
    "\n",
    "    ## get the offset of the first frame keypoints from the center of the image\n",
    "    center_x, center_y = find_first_frame_center(block)\n",
    "    displacement_x = image_center_x - center_x\n",
    "    displacement_y = image_center_y - center_y\n",
    "    \n",
    "    displacement_x, displacement_y = adjust_bounds(block, x_dim, y_dim, displacement_x, displacement_y)\n",
    "\n",
    "    return displacement_x, displacement_y\n",
    "\n",
    "def recenter_block (block, displacement_x, displacement_y):\n",
    "    \n",
    "    for e, frame in enumerate(block):\n",
    "        if e == 0:\n",
    "            print(f'adjusting block by {displacement_x}, {displacement_y}')\n",
    "\n",
    "        frame_keypoints = np.array(block[e]['first_detection_keypoints'])\n",
    "        frame_keypoints[:, 0] += displacement_x\n",
    "        frame_keypoints[:, 1] += displacement_y\n",
    "\n",
    "        block[e]['first_detection_keypoints'] = frame_keypoints.tolist()\n",
    "    \n",
    "    return block\n",
    "\n",
    "def reorder_keypoints(keypoints, mapping):\n",
    "    \"\"\"\n",
    "    Reorders the keypoints based on the mapping.\n",
    "    \n",
    "    :param new: List of keypoints to be reordered.\n",
    "    :param mapping: Dictionary with the new index for each old index.\n",
    "    :return: Reordered list of keypoints.\n",
    "    \"\"\"\n",
    "    reordered = []\n",
    "    reordered.extend([[0, 0] for i in range(len(keypoints) + 2)])\n",
    "    \n",
    "    reordered[1][0] = (keypoints[5][0]+keypoints[6][0])/2\n",
    "    reordered[1][1] = (keypoints[5][1]+keypoints[6][1])/2\n",
    "\n",
    "    reordered[8][0] = (keypoints[11][0]+keypoints[12][0])/2\n",
    "    reordered[8][1] = (keypoints[11][1]+keypoints[12][1])/2\n",
    "    \n",
    "    # Ensure 'mapping' does not contain indices higher than 18.\n",
    "    for old_index, new_index in mapping.items():\n",
    "        try:\n",
    "            reordered[new_index][0] = keypoints[old_index][0]\n",
    "            reordered[new_index][1] = keypoints[old_index][1]\n",
    "        except IndexError:\n",
    "            print(f'{old_index, new_index} is out of range, skipping.')\n",
    "\n",
    "    return reordered\n",
    "\n",
    "\n",
    "def rotate_block(block):\n",
    "    \"\"\"\n",
    "    Rotate keypoints by a given angle.\n",
    "    \n",
    "    :param keypoints: np.array of shape (N, 2) representing the keypoints.\n",
    "    :param angle: Rotation angle in radians.\n",
    "    :return: Rotated keypoints as np.array of shape (N, 2).\n",
    "    \"\"\"\n",
    "    angle = find_first_frame_rotation(block)\n",
    "\n",
    "    rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "                                [np.sin(angle),  np.cos(angle)]])\n",
    "    \n",
    "    # Rotate keypoints\n",
    "    for e, frame in enumerate(block):\n",
    "        if e == 0:\n",
    "            print(f'rotating block by {angle}')\n",
    "\n",
    "        frame_keypoints = np.array(block[e]['first_detection_keypoints'])\n",
    "        frame_keypoints = np.dot(frame_keypoints, rotation_matrix)\n",
    "\n",
    "        block[e]['first_detection_keypoints'] = frame_keypoints.tolist()\n",
    "    \n",
    "    return block\n",
    "\n",
    "\n",
    "def normalize_skeleton(keypoints, frame_index=0):\n",
    "\n",
    "    keypoints = reorder_keypoints(keypoints, mapping)\n",
    "    keypoints = np.array(keypoints)\n",
    "\n",
    "\n",
    "    return keypoints\n",
    "\n",
    "def update_frame(frame_index, block, ax, limb_sequence, block_number):\n",
    "    ax.clear()\n",
    "    pose = block[frame_index]['first_detection_keypoints']\n",
    "    pose_norm = normalize_skeleton(pose,frame_index=frame_index)\n",
    "\n",
    "    x = pose_norm[:, 0]\n",
    "    y = pose_norm[:, 1]\n",
    "\n",
    "    skpt = []\n",
    "    \n",
    "    for j in range(len(pose_norm)):\n",
    "        skpt.extend([x[j].astype(float),y[j].astype(float),2.0])   \n",
    "    #skpt.extend([0,0,2,0,0,2])\n",
    "\n",
    "    skpt_json_format = {\"annotations\": skpt}\n",
    "\n",
    "    with open(os.path.join('/workspaces/wiggleformer/data/PANDA/test/test_frame_jsons', f'{block_number}_{frame_index}.json'), \"w\") as json_file:\n",
    "        json.dump(skpt_json_format, json_file)\n",
    "\n",
    "    plot_skeleton(pose_norm, limb_sequence, ax)\n",
    "    #ax.set_xlim([-1, 1])\n",
    "    #ax.set_ylim([0, 3])\n",
    "    plt.axis('off')\n",
    "\n",
    "    if not os.path.exists('/workspaces/wiggleformer/data/PANDA/test/test_frame_images'):\n",
    "        os.makedirs('/workspaces/wiggleformer/data/PANDA/test/test_frame_images')\n",
    "\n",
    "    plt.savefig(f'/workspaces/wiggleformer/data/PANDA/test/test_frame_images/{block_number}_{frame_index}.png')\n",
    "    #ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "def plot_skeleton(keypoints, limb_sequence, ax):\n",
    "    keypoints = np.array(keypoints)\n",
    "    \n",
    "    x = keypoints[:, 0]\n",
    "    y = keypoints[:, 1]\n",
    "    for i, limb in enumerate(limb_sequence):\n",
    "        start_idx, end_idx = limb\n",
    "        ax.plot([x[start_idx], x[end_idx]], [y[start_idx], y[end_idx]], linewidth=5, color=[j/255 for j in colors[i]], alpha=alpha)\n",
    "    for i in range(len(keypoints)):\n",
    "        ax.plot(x[i], y[i], 'o', markersize=10, color=[j/255 for j in colors[i]], alpha=alpha)\n",
    "\n",
    "def animate_good_block(block, limb_sequence, block_number):\n",
    "    dpi = 90\n",
    "    x_dim, y_dim = get_orig_dimensions('/workspaces/wiggleformer/data/PANDA/test/video/test_clip.mp4')\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(x_dim/dpi, y_dim/dpi)\n",
    "    fig.set_dpi(dpi)\n",
    "\n",
    "    ani = FuncAnimation(fig, update_frame, frames=range(len(block)), fargs=(block, ax, limb_sequence, block_number), interval=100)\n",
    "    return ani\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `good_blocks` is a list of good blocks\n",
    "dpi = 90\n",
    "alpha = 0.7\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "x_dim, y_dim, x_center, y_center = get_orig_dimensions('/workspaces/wiggleformer/data/PANDA/test/video/test_clip.mp4')\n",
    "\n",
    "print(x_dim, y_dim)\n",
    "ax.set_xlim([0, x_dim])\n",
    "ax.set_ylim([0, y_dim])\n",
    "ax.invert_yaxis()\n",
    "ax.axis('off')\n",
    "\n",
    "fig.set_size_inches(x_dim/dpi, y_dim/dpi)\n",
    "fig.set_dpi(dpi)\n",
    "\n",
    "\n",
    "### \n",
    "\"\"\"\n",
    "for each movement block, get the displacement of the first frame\n",
    "check if the displacement will make any x or y value in any frame go out of bounds\n",
    "if so, adjust the displacement accordingly\n",
    "return the displacement parameters \n",
    "recenter all the blocks using the displacement parameters\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "for i, block in enumerate(movement_blocks[2:3]):          \n",
    "    \n",
    "    block = rotate_block(block)\n",
    "    displacement_x, displacement_y = get_block_displacement(block, x_center, y_center, x_dim, y_dim)\n",
    "    #ani = animate_good_block(block, limb_sequence, block_number=i)  \n",
    "    #ani.save(f'animation_{i}.mp4', writer='ffmpeg', codec='mpeg4')\n",
    "    block = recenter_block(block, displacement_x, displacement_y)\n",
    "    \n",
    "    for i, frame in enumerate(block):\n",
    "        pose = block[i]['first_detection_keypoints']\n",
    "        pose_norm = normalize_skeleton(pose)\n",
    "\n",
    "        kpts = plot_skeleton(pose_norm, limb_sequence, ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ang = np.arctan2(pose[8][0], pose[8][1])\n",
    "center = (np.array(pose[8]) + np.array(pose[1]))/2\n",
    "keypoints = [[0 + pose[i][0]*np.cos(ang) - pose[i][1]*np.sin(ang), 0 + pose[i][0]*np.sin(ang) + pose[i][1]*np.cos(ang)] for i in range(len(pose)) ]\n",
    "#keypoints = [[keypoints[i][0] - center[0], keypoints[i][1] - center[1]] for i in range(len(keypoints))]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.invert_yaxis()\n",
    "plt.scatter(np.array(keypoints)[:,0],np.array(keypoints)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_openpose = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0],\n",
    "          [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255],\n",
    "          [85, 0, 255], \\\n",
    "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85],[255, 0, 0]]\n",
    "\n",
    "orig = [\n",
    "540.3536987304688, 342.3381652832031, 2.0, \n",
    "542.3785705566406, 464.4452209472656, 2.0, \n",
    "398.61273193359375, 471.8660888671875, 2.0, \n",
    "309.51837158203125, 573.059814453125, 2.0, \n",
    "206.92486572265625, 639.1730346679688, 2.0, \n",
    "686.1444091796875, 457.02435302734375, 2.0, \n",
    "786.0380859375, 452.9765930175781, 2.0, \n",
    "765.789306640625, 340.9889221191406, 2.0, \n",
    "541.9588297041303, 887.6578947368421, 2.0, \n",
    "440.11672444097235, 871.078947368421, 2.0, \n",
    "347.3159484863281, 936.0078125, 2.0, \n",
    "451.25933837890625, 1066.885009765625, 2.0, \n",
    "643.8009349672882, 904.2368421052631, 2.0, \n",
    "693.5377770725513, 1029.7631578947369, 2.0, \n",
    "571.4017333984375, 1127.6011962890625, 2.0, \n",
    "486.35711669921875, 311.3054504394531, 2.0, \n",
    "583.5509643554688, 304.5592041015625, 2.0, \n",
    "406.7121887207031, 336.941162109375, 2.0, \n",
    "646.9968872070312, 326.1471862792969, 2.0]\n",
    "\n",
    "x = orig[::3]\n",
    "y = orig[1::3]\n",
    "\n",
    "limbSeq_openpose = [[0,15],[0,16],[15,16],[15,17],[16,18],[1,2],[2,3],[3,4],[1,5],[5,6],[6,7],[2,9],[5,12],[8,9],[9,10],[10,11],[8,12],[12,13],[13,14]]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, limb in enumerate(limbSeq_openpose):\n",
    "       start_idx, end_idx = limb\n",
    "       ax.plot([x[start_idx], x[end_idx]], [y[start_idx], y[end_idx]], linewidth=5, color=[j/255 for j in colors_openpose[i]], alpha=alpha)\n",
    "\n",
    "# Annotating each node with its number\n",
    "for i in range(len(x)):\n",
    "    ax.text(x[i], y[i], str(i), color='black', fontsize=12)\n",
    "\n",
    "ax.scatter(x, y, color='black')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0:0,1:15,2:16,3:17,4:18,5:2,6:5,7:3,8:6,9:4,10:7,11:9,12:12,13:10,14:13,15:11,16:14,17:17,18:18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0:0,1:15,2:16,3:17,4:18,5:2,6:5,7:3,8:6,9:4,10:7,11:9,12:12,13:10,14:13,15:11,16:14, 17:1, 18:8}\n",
    "\n",
    "new = [320.50829107033337, 433.67906792050724, 2.0, \n",
    "       274.1088263974856, 449.43478431734854, 2.0, \n",
    "       330.9267882971945, 468.3157080128923, 2.0, \n",
    "       182.85167823210918, 437.8879811881567, 2.0, \n",
    "       357.59422791013975, 476.12261263294806, 2.0, \n",
    "       171.02284494584126, 365.1858283040423, 2.0, \n",
    "       349.1798307782122, 384.1887028336272, 2.0, \n",
    "       106.88257222489528, 281.48226430901593, 2.0, \n",
    "       385.76217541370863, 338.38431838869735, 2.0, \n",
    "       159.65307815993583, 220.35466777669023, 2.0, \n",
    "       426.36923733174285, 311.7680972331101, 2.0, \n",
    "       199.86674724556462, 135.99158458729664, 2.0, \n",
    "       296.5669342497698, 151.9616892276008, 2.0, \n",
    "       110.49486961596301, 174.3002588871683, 2.0, \n",
    "       348.242014805116, 118.76372336422025, 2.0, \n",
    "       163.04151219676373, 66.44034328851353, 2.0, \n",
    "       266.2557857576525, 0.0, 2.0,\n",
    "       0,0,2,\n",
    "       0,0,2]\n",
    "\n",
    "new[17*3] = (new[5*3]+new[6*3])/2\n",
    "new[17*3+1] = (new[5*3+1]+new[6*3+1])/2\n",
    "\n",
    "new[18*3] = (new[11*3]+new[12*3])/2\n",
    "new[18*3+1] = (new[11*3+1]+new[12*3+1])/2\n",
    "\n",
    "x = new[::3]\n",
    "y = new[1::3]\n",
    "\n",
    "for i,limb in enumerate(limb_sequence):\n",
    "    \n",
    "    start_idx, end_idx = limb\n",
    "    plt.plot([x[start_idx], x[end_idx]], [y[start_idx], y[end_idx]],linewidth=5, color=[j/255 for j in colors_openpose[i]], alpha=alpha)\n",
    "\n",
    "plt.scatter(new[::3], new[1::3], color='black')\n",
    "for i in range(len(x)):\n",
    "    plt.text(new[::3][i], new[1::3][i], str(i), color='black', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limbSeq_openpose = [[0,15],[0,16],[15,16],[15,17],[16,18],[1,2],[2,3],[3,4],[1,5],[5,6],[6,7],[2,9],[5,12],[8,9],[9,10],[10,11],[8,12],[12,13],[13,14]]\n",
    "\n",
    "def reorder_keypoints(new, mapping):\n",
    "    \"\"\"\n",
    "    Reorders the keypoints based on the mapping.\n",
    "    \n",
    "    :param new: List of keypoints to be reordered.\n",
    "    :param mapping: Dictionary with the new index for each old index.\n",
    "    :return: Reordered list of keypoints.\n",
    "    \"\"\"\n",
    "    reordered = [0] * len(new)  # Create a list with the same length as 'new'\n",
    "\n",
    "    for old_index, new_index in mapping.items():\n",
    "        # Multiply by 3 since each keypoint consists of 3 values (x, y, and a score)\n",
    "        old_position = old_index * 3\n",
    "        new_position = new_index * 3\n",
    "\n",
    "        reordered[new_position:new_position+3] = new[old_position:old_position+3]\n",
    "    \n",
    "    return reordered\n",
    "\n",
    "# Reorder the list based on the mapping\n",
    "reordered = [0] * len(new)  # Create a list with the same length as 'new'\n",
    "\n",
    "for old_index, new_index in mapping.items():\n",
    "    # Multiply by 3 since each keypoint consists of 3 values (x, y, and a score)\n",
    "    old_position = old_index * 3\n",
    "    new_position = new_index * 3\n",
    "\n",
    "    reordered[new_position:new_position+3] = new[old_position:old_position+3]\n",
    "\n",
    "x = reordered[::3]\n",
    "y = reordered[1::3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, limb in enumerate(limbSeq_openpose):\n",
    "       start_idx, end_idx = limb\n",
    "       ax.plot([x[start_idx], x[end_idx]], [y[start_idx], y[end_idx]], linewidth=5, color=[j/255 for j in colors_openpose[i]], alpha=alpha)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    ax.text(x[i], y[i], str(i), color='black', fontsize=12)\n",
    "\n",
    "ax.scatter(x, y, color='black')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import autosklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "\n",
    "import autosklearn.classification\n",
    "import autosklearn.metrics\n",
    "\n",
    "features = pd.read_pickle('/workspaces/wiggleformer/data/PANDA/pose_estimates/final_feature_set.pkl')\n",
    "holdout_ids = pd.read_csv('/workspaces/wiggleformer/data/PANDA/holdout_ids.csv')\n",
    "\n",
    "features = features.loc[features['category'] == 1]\n",
    "features = features.loc[features['age_in_weeks'] > 0]\n",
    "\n",
    "features['id'] = features['infant'].str.split('_').str[1]\n",
    "features = features.loc[~features['id'].isin(holdout_ids['gma_id'])]\n",
    "\n",
    "pivot_df = features.pivot_table(index='infant', columns=['feature'], values='Value', fill_value=0)\n",
    "risk_df = features[['infant', 'risk']].drop_duplicates()\n",
    "\n",
    "# Merge the new risk_df DataFrame with pivot_df\n",
    "merged_df = pd.merge(pivot_df, risk_df, on='infant', how='inner')\n",
    "\n",
    "X = merged_df.drop(columns=['infant', 'risk'])\n",
    "y = merged_df['risk']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "train = pd.concat([X_train,y_train],axis=1)\n",
    "test = pd.concat([X_test,y_test],axis=1)\n",
    "\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=120,\n",
    "    metric=[autosklearn.metrics.precision, autosklearn.metrics.recall],\n",
    "    delete_tmp_folder_after_terminate=False,\n",
    "    memory_limit=None\n",
    ")\n",
    "automl.fit(X_train, y_train, dataset_name=\"infant movement\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
